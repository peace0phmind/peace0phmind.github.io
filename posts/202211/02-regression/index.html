<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.106.0"><link rel="shortcut icon" type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/images/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/images/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/images/icons/apple_touch_icon_next.png><meta itemprop=name content="Regression"><meta itemprop=description content="regression"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://peace0phmind.github.io/images/avatar.png"><meta itemprop=keywords content="regression"><meta property="og:type" content="article"><meta property="og:title" content="Regression"><meta property="og:description" content="regression"><meta property="og:image" content="/images/avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="https://peace0phmind.github.io/posts/202211/02-regression/"><meta property="og:site_name" content="Mind's Home"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="peace0phmind"><meta property="article:published_time" content="2022-11-08 14:27:33 +0800 +0800"><meta property="article:modified_time" content="2022-11-08 14:27:33 +0800 +0800"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.dabf51c13eb53dd19c627065c0fe53422155aaa07efc790ec29ea1d5b51707b8.css><style type=text/css>.post-footer{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){console.log("aaaa hello world!");const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"02-regression","permalink":"https://peace0phmind.github.io/posts/202211/02-regression/","title":"Regression","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>Regression - Mind's Home</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class="header not-print" itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Mind's Home</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Notebook</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#使用简单模型>使用简单模型</a><ul><li><a href=#function-with-unknown-parameters>Function with Unknown Parameters</a></li><li><a href=#define-loss-from-training-data>Define Loss from Training Data</a></li><li><a href=#optimization>Optimization</a><ul><li><a href=#简述gradient-descent过程>简述<code>Gradient Descent</code>过程</a></li><li><a href=#使用单参数的多个连续的历史记录>使用单参数的多个连续的历史记录</a></li><li><a href=#名词解释>名词解释</a></li></ul></li><li><a href=#总结machine-learning训练的简单步骤>总结Machine Learning训练的简单步骤</a></li></ul></li><li><a href=#打破模型局限>打破模型局限</a><ul><li><a href=#all-piecewise-linear-curves>All Piecewise Linear Curves</a></li><li><a href=#sigmoid>sigmoid</a></li><li><a href=#loss-function>Loss function</a></li><li><a href=#optimization-of-new-model>Optimization of New Model</a></li><li><a href=#使用-sigmoid-rightarrow-relu->使用$ Sigmoid \rightarrow ReLU $</a><ul><li><a href=#作个数不同的relu>作个数不同的ReLU</a></li><li><a href=#作多层relu>作多层ReLU</a></li></ul></li></ul></li><li><a href=#backpropagation>Backpropagation</a><ul><li><a href=#gradient-descent>Gradient Descent</a></li><li><a href=#chain-rule>Chain Rule</a></li><li><a href=#forward-and-backward-pass>Forward and Backward pass:</a></li></ul></li><li><a href=#regularization>Regularization</a></li><li><a href=#reference-video>Reference Video</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=peace0phmind src=/images/img-lazy-loading.gif data-src=/images/avatar.png><p class=site-author-name itemprop=name>peace0phmind</p><div class=site-description itemprop=description></div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/posts/><span class=site-state-item-count>31</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>2</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>11</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/peace0phmind title="Github → https://github.com/peace0phmind" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2020-10-20T19:37:14+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=25790></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=69></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-11-20T11:22:00+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class="tool-buttons not-print"><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a>
<a href=https://github.com/peace0phmind rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://peace0phmind.github.io/posts/202211/02-regression/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/images/avatar.png"><meta itemprop=name content="peace0phmind"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="peace0phmind"><meta itemprop=description content></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="Regression"><meta itemprop=description content="regression"></span><header class="post-header not-print"><h1 class=post-title itemprop="name headline">Regression
<a href=https://github.com/peace0phmind/peace0phmind.github.io/tree/master/content/posts/202211/02-regression.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-11-08 14:27:33 +0800 +0800" itemprop="dateCreated datePublished" datetime="2022-11-08 14:27:33 +0800 +0800">2022-11-08</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/machine-learning itemprop=url rel=index><span itemprop=name>machine-learning</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>2518</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>6分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/posts/202211/02-regression/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body print autonumber" itemprop=articleBody><p><code>Regression</code>: Input a vector, the function outputs a scalar.</p><h2 id=使用简单模型>使用简单模型</h2><p>预测问题：根据前面的浏览数据，预测后面的浏览量</p><h3 id=function-with-unknown-parameters>Function with Unknown Parameters</h3><p><code>Model</code>: $y = b + wx_1$<br>$y$(<code>Label</code>): no. of views on 2/26， $x_1$(<code>feature</code>): no. of views on 2/25<br>$w$(<code>weight</code>) and $b$(<code>bias</code>) are unknown parameters (learned from data)<br></p><h3 id=define-loss-from-training-data>Define Loss from Training Data</h3><ul><li>Loss is a function of parameters: $L(b, w)$<br></li><li>Loss: how good a set of values is.<br></li><li>Loss: $ L = \frac{1}{N}\sum\limits_{n=1}^N e_n$<br><ul><li>$e = |y - \hat{y}|$ $L$ is mean absolute error (MAE)<br></li><li>$e = (y-\hat{y})^2$ $L$ is mean square error (MSE)<br></li><li>if $y$ and $\hat{y}$ are both probability distributions, then use <code>Cross-Entropy</code></li></ul></li></ul><p>使用不同的参数，计算出来的Loss画出来的等高线图叫做：<code>Error Surface</code><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.013.jpg alt=等高线图 title="等高线图: Error Surface"><figcaption>等高线图: Error Surface</figcaption></figure></p><h3 id=optimization>Optimization</h3><p>找一个$w$和$b$，使$L$最小： $ w^*, b^* = arg \min\limits_{w, b} L$
这种找到最小$w$和$b$的方法叫做：<code>Gradient Descent</code></p><h4 id=简述gradient-descent过程>简述<code>Gradient Descent</code>过程</h4><p>以一个参数$w$为例描述<code>Gradient Descent</code>的过程:</p><ul><li>随机初始化点$w^0$</li><li>计算$w=w^0$时，对$L$的微分是多少：$\frac{{\partial}L}{{\partial}W}|_{w=w^0}$<ul><li>如果计算出来的结果为负数，则增加$w$</li><li>如果计算出来的结果为正数，则减少$w$</li><li>增加或减少的数值为：${\color{red}\eta}\frac{{\partial}L}{{\partial}W}|_{w=w^0}$, $\color{red}\eta$:叫<span style=color:red>learning rate</span>,是一个hyperparameter</li><li>这个过程用的数学表达式是：$ w^1 \leftarrow w^0 - {\color{red}\eta}\frac{{\partial}L}{{\partial}W}|_{w=w^0} $</li><li>重复上述步骤不断更新$w$。两种状况会停下来:<ul><li>更新的次数达到预设值</li><li>微分为0</li></ul></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.016.jpg alt title="Gradient Descent"><figcaption>Gradient Descent</figcaption></figure></p><p>当两个参数$w$, $b$时:</p><ul><li>随机初始化$w^0$, $b^0$</li><li>计算两个微分值<ul><li>$ \frac{\partial L}{\partial w}|_{w=w^0, b=b^0} $ 用$w$的微分更新$w$的值</li><li>$ w^1 \leftarrow w^0 - {\color{red}\eta}\frac{\partial L}{\partial w}|_{w=w^0, b=b^0} $</li><li>$ \frac{\partial L}{\partial b}|_{w=w^0, b=b^0} $ 用$b$的微分更新$b$的值</li><li>$ b^1 \leftarrow b^0 - {\color{red}\eta}\frac{\partial L}{\partial b}|_{w=w^0, b=b^0} $</li><li>Update $w$ and $b$ interatively</li></ul></li></ul><h4 id=使用单参数的多个连续的历史记录>使用单参数的多个连续的历史记录</h4><p>通过观察资料发现数据有7天为一个周期，所以使用新的公式进行调整, 并得到下面数据：</p><table><thead><tr><th>days</th><th>function</th><th>training loss</th><th>testing loss</th></tr></thead><tbody><tr><td>1</td><td>$ y = b + wx_{\color{red}1} $</td><td>$ L = 0.48k $</td><td>$ L&rsquo; = 0.58k $</td></tr><tr><td>7</td><td>$ y = b + \sum\limits_{j=1}^{\color{red}7}w_jx_j $</td><td>$ L = 0.38k $</td><td>$ L&rsquo; = 0.49k $</td></tr><tr><td>28</td><td>$ y = b + \sum\limits_{j=1}^{\color{red}28}w_jx_j $</td><td>$ L = 0.33k $</td><td>$ L&rsquo; = 0.46k $</td></tr><tr><td>56</td><td>$ y = b + \sum\limits_{j=1}^{\color{red}56}w_jx_j $</td><td>$ L = 0.32k $</td><td>$ L&rsquo; = 0.46k $</td></tr></tbody></table><p>上述模型有个共同的名字<code>Linear Models</code></p><span style=color:red>调整模型参数，观察Training Loss和Testing Loss的变化，挑选合适的模型</span><h4 id=名词解释>名词解释</h4><p><code>hyperparameter</code>: 需要人来设置的参数</p><p><code>local minima</code>: 局部最小值</p><p><code>global minima</code>: 全局最小值</p><h3 id=总结machine-learning训练的简单步骤>总结Machine Learning训练的简单步骤</h3><ul><li>function with unknown</li><li>define loss from training data</li><li>optimization</li></ul><h2 id=打破模型局限>打破模型局限</h2><p>不同的w和不同的b对Linear Models的影响如蓝色线。红色表示可能的真实趋势。这种来自于Model的限制叫做<code>Model Bias</code>。<figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.023.jpg alt="Linear Models的局限性" title="Linear Models的局限性"><figcaption>Linear Models的局限性</figcaption></figure></p><h3 id=all-piecewise-linear-curves>All Piecewise Linear Curves</h3><p>All Piecewise Linear Curves = constant + sum of a set of <code>Hard Sigmoid</code><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.025.jpg alt="Piecewise Linear Curves" title="Piecewise Linear Curves"><figcaption>Piecewise Linear Curves</figcaption></figure></p><h3 id=sigmoid>sigmoid</h3><p>\begin{align*}
y &= {\color{red}c}\frac{1}{1+e^{-({\color{green}b}+{\color{blue}w}x_1)}} \cr
&= {\color{red}c}\,sigmoid({\color{green}b}+{\color{blue}w}x_1)
\end{align*}</p><p>调整$ {\color{blue}w}, {\color{green}b}, {\color{red}c} $对应的函数图像<figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.028.jpg alt="Sigmoid Parameters" title="Sigmoid Parameters"><figcaption>Sigmoid Parameters</figcaption></figure></p><p>相对于红色线段，可以用多个<code>Sigmoid</code>函数组合出来，将0:<code>constant</code>和1,2,3<code>sigmoid</code>加起来就是红色线段</p><p>\begin{align*}
b\tag{0}\cr
{\color{red}c_1}\,sigmoid({\color{green}b_1}+{\color{blue}w_1}x_1)\tag{1}\cr
{\color{red}c_2}\,sigmoid({\color{green}b_2}+{\color{blue}w_2}x_1)\tag{2}\cr
{\color{red}c_3}\,sigmoid({\color{green}b_3}+{\color{blue}w_3}x_1)\tag{3}\cr
\textcolor{red}{\text{red curve}}\text{ will be:} \cr
y = b + \sum_{i=1}^3{\color{red}c_i}\,sigmoid({\color{green}b_i}+{\color{blue}w_i}x1)
\end{align*}</p><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.029.jpg alt="Sigmoid Parameters" title="Sigmoid Parameters"><figcaption>Sigmoid Parameters</figcaption></figure></p><p>基于sigmoid的模型，对原来的模型进行调整如下：
\begin{align*}
y &= b+wx_1 \cr
& \Downarrow \cr
y &= b + \sum_{i=1}^n {\color{red}c_i} \, sigmoid({\color{green}b_i}+{\color{blue}w_i}x_i) \cr
y &= b + \sum_{j=1}^m w_jx_j \cr
& \Downarrow \cr
y &= b + \sum_{i=1}^n {\color{red}c_i} \, sigmoid({\color{green}b_i}+\sum_{j=1}^m{\color{blue}w_{ij}}x_j)
\end{align*}</p><p>使$n=3, m=3$对表达式$y = b + \sum_{i=1}^n {\color{red}c_i} \, sigmoid({\color{green}b_i}+\sum_{j=1}^m{\color{blue}w_{ij}}x_j)$进行展开
\begin{align*}
r_1 &= {\color{green}b_1} + {\color{blue}w_{11}}x_1 + {\color{blue}w_{12}}x_2 + {\color{blue}w_{13}}x_3 \cr
r_2 &= {\color{green}b_2} + {\color{blue}w_{21}}x_1 + {\color{blue}w_{22}}x_2 + {\color{blue}w_{23}}x_3 \cr
r_3 &= {\color{green}b_3} + {\color{blue}w_{31}}x_1 + {\color{blue}w_{32}}x_2 + {\color{blue}w_{33}}x_3 \cr
&\Downarrow \cr
\begin{bmatrix} r_1 \cr r_2 \cr r_3 \end{bmatrix} &=
\begin{bmatrix} {\color{green}b_1} \cr {\color{green}b_2} \cr {\color{green}b_3} \end{bmatrix} +
\begin{bmatrix}
{\color{blue}w_{11}} & {\color{blue}w_{12}} & {\color{blue}w_{13}} \cr
{\color{blue}w_{21}} & {\color{blue}w_{22}} & {\color{blue}w_{23}} \cr
{\color{blue}w_{31}} & {\color{blue}w_{32}} & {\color{blue}w_{33}} \cr
\end{bmatrix}
\begin{bmatrix} x_1 \cr x_2 \cr x_3 \end{bmatrix}
\end{align*}</p><p>其中$\color{red}\sigma$表示<code>sigmoid</code>表达式<figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.037.jpg alt=展开图示 title="Sigmoid 展开图示"><figcaption>Sigmoid 展开图示</figcaption></figure></p><p>如下图所示，x为<code>feature</code>；而所有的$W, {\color{green}b}, c^T, b$作为unknown parameters展开为一个长的一维向量，定义为$\color{red}\theta$<figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.038.jpg alt="unknown parameters" title="unknown parameters"><figcaption>unknown parameters</figcaption></figure></p><h3 id=loss-function>Loss function</h3><ul><li>Loss is a function of parameters $L(\theta)$</li><li>Loss means how good a set of values is.</li></ul><h3 id=optimization-of-new-model>Optimization of New Model</h3><p>$ \theta^* = arg\,\min\limits_\theta L$</p><ul><li>(Randomly) Pick initial values $\theta^0$</li><li>对所有参数$\theta$对$L$做微分，这里的$g$叫做<code>gradient</code></li></ul><div>\begin{align}
gradient \Leftarrow g &=
\begin{bmatrix}
{\frac{\partial L}{\partial\theta_1}|_{\theta=\theta^0}} \cr
{\frac{\partial L}{\partial\theta_2}|_{\theta=\theta^0}} \cr
\vdots
\end{bmatrix} \cr
g &= \nabla L(\theta^0)
\end{align}</div><ul><li>然后进行参数更新</li></ul><div>$$
\begin{bmatrix}
\theta_1^1 \cr \theta_2^1 \cr \vdots
\end{bmatrix}
\leftarrow
\begin{bmatrix}
\theta_1^0 \cr \theta_2^0 \cr \vdots
\end{bmatrix}
-
\begin{bmatrix}
{\color{red}\eta}\frac{\partial L}{\partial\theta_1}|_{\theta=\theta^0} \cr
{\color{red}\eta}\frac{\partial L}{\partial\theta_2}|_{\theta=\theta^0} \cr
\vdots
\end{bmatrix}
$$
$$
\theta^1 \leftarrow \theta^0 - {\color{red}\eta}g
$$</div><ul><li>Compute gradient $ g = \nabla L(\theta^0) $</li></ul><p>全部资料是$L$,批次编号为$L^1, L^2, L^3$。<span style=color:red>batch</span>是进行参数更新的单位，即一个批次进行一次参数更新；<span style=color:red>epoch</span>表示所有批次全部执行了参数更新。</p><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.044.jpg alt="batch and epoch" title="batch and epoch"><figcaption>batch and epoch</figcaption></figure></p><h3 id=使用-sigmoid-rightarrow-relu->使用$ Sigmoid \rightarrow ReLU $</h3><ul><li><code>Rectified Linear Unit (ReLU)</code>: $ {\color{red}c}\,max(0, {\color{green}b} + {\color{blue}w}x_1) $</li><li>类似<code>sigmoid</code>和<code>ReLU</code>的函数在机器学习中叫做<span style=color:red><code>Activation function</code></span></li><li>1个<code>sigmoid</code>图形需要2个<code>ReLU</code>来表示</li></ul><h4 id=作个数不同的relu>作个数不同的ReLU</h4><ul><li>only one layer</li><li>input features are the no. of views in the past 56 days</li></ul><table><thead><tr><th>model</th><th>training loss</th><th>testing loss</th></tr></thead><tbody><tr><td>linear</td><td>0.32k</td><td>0.46k</td></tr><tr><td>10ReLU</td><td>0.32k</td><td>0.45k</td></tr><tr><td>100ReLU</td><td>0.28k</td><td>0.43k</td></tr><tr><td>1000ReLU</td><td>0.27k</td><td>0.43k</td></tr></tbody></table><h4 id=作多层relu>作多层ReLU</h4><ul><li>100 ReLU for each layer</li><li>input features are the no. of views in the past 56 days</li><li><code>Better on training data, worse on unseen data</code>: <span style=color:red><code>Overfittin</code></span> , see layer count 4.</li></ul><table><thead><tr><th>layer count</th><th>training loss</th><th>testing loss</th></tr></thead><tbody><tr><td>1</td><td>0.28k</td><td>0.43k</td></tr><tr><td>2</td><td>0.18k</td><td>0.39k</td></tr><tr><td>3</td><td>0.14k</td><td>0.38k</td></tr><tr><td>4</td><td>0.10k</td><td>0.44k</td></tr></tbody></table><h2 id=backpropagation>Backpropagation</h2><p><code>Backpropgation</code>: an efficient way to compute $\sfrac{\partial L}{\partial w}$</p><h3 id=gradient-descent>Gradient Descent</h3><ul><li>对每一个参数针对L进行偏微分得到: $\nabla L(\theta)$</li><li>使用<code>batch</code>的数据对参数$\theta$进行更新.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/07.002.jpg alt="Gradient Descent" title="Gradient Descent"><figcaption>Gradient Descent</figcaption></figure></p><h3 id=chain-rule>Chain Rule</h3><ul><li>case 1: $\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$</li><li>case 2: $\frac{dz}{ds}=\frac{dz}{dx}\frac{dx}{ds}+\frac{dz}{dy}\frac{dy}{ds}$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/07.003.jpg alt="Chain Rule" title="Chain Rule"><figcaption>Chain Rule</figcaption></figure></p><h3 id=forward-and-backward-pass>Forward and Backward pass:</h3><ul><li><code>Forward pass</code>: Compute $\sfrac{\partial z}{\partial w}$for all parameters</li><li><code>Backward pass</code>: Compute $\sfrac{\partial C}{\partial z}$ for all activation function inputs z</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/07.005.jpg alt="Forward and Backward pass" title="Forward and Backward pass"><figcaption>Forward and Backward pass</figcaption></figure></p><h2 id=regularization>Regularization</h2><p><code>Regularization</code>出现的背景：当原始数据有过多的feature和模型有大量的w，可能存在某些feature确实与最终的结果无关，在这种情况下可以先将所有feature包含进来，然后通过<code>Regularization</code>的思路对w进行优化，从而降低无效feature对最终结果的影响。</p><ul><li>假设Model Function为$ y = b + \sum w_i x_i $</li><li>原来定义的Loss Function为$ L = \sum\limits_n( \hat{y}^n - ( b + \sum w_i x_i ) )^2$</li><li>Regularization既是在上面Loss Function的基础上加上红色部分: $ L = \sum\limits_n( \hat{y}^n - ( b + \sum w_i x_i ) )^2 + \color{red}\lambda\sum(w_i)^2$</li><li><span style=color:red>The functions with smaller $w_i$ are better</span></li><li>Why smooth functions are preferred?<ul><li>If some noises corrupt input $x_i$ when testing. A smoother function has less influence.</li><li>因为在进行预测时，有噪音输入的情况下，越smooth的function对输出造成的影响越不敏感。</li></ul></li><li><span style=color:red>其中的$\lambda$也是一个hyperparameter</span></li></ul><p>当我们调整$\lambda$的值，观察Loss的变化是，我们可以观察到如下信息：</p><ul><li>$\lambda$越大时，Regularization项影响就越大，整个function就越平滑</li><li>Training随着$\lambda$的增加而增加</li><li>Testing随着$\lambda$的增加先减少再增加</li><li>We prefer smooth function, but don&rsquo;t be too smooth.</li><li>所以$\lambda$的选择值选择在Testing的Loss的转折点处</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/02-regression/01.035.jpg alt="Regularization Loss" title="Regularization Loss"><figcaption>Regularization Loss</figcaption></figure></p><h2 id=reference-video>Reference Video</h2><p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/Ye018rCVvOo style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/bHcJCp2Fyxs style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/ibJpTrp5mcE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/fegAeph9UaA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p></div><footer class="post-footer not-print"><div class=post-tags><a href=/tags/regression>regression</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/202211/03-classification/ rel=next title=Classification><i class="fa fa-chevron-left"></i> Classification</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/202211/mpv-config/ rel=prev title=Mpv常用配置>Mpv常用配置
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class="footer not-print"><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>peace0phmind</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"https://peace0phmind.github.io","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.0","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.c000895f35c4f549795bbb87a678888fa0246f9513d456814f0988441a9a4b7c.js defer></script>
<script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin=anonymous></script>
<script type=text/javascript>mediumZoom("[data-zoomable]",{background:null})</script><script type=text/javascript>window.MathJax={options:{ignoreHtmlClass:"markmap"},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!1,packages:{"[+]":["base","extpfeil","ams","amscd","newcommand","knowl","sfrac"]}},chtml:{},loader:{load:["input/asciimath","[tex]/extpfeil","[tex]/amscd","[tex]/newcommand","[pretext]/mathjaxknowl3.js"],paths:{pretext:"https://pretextbook.org/js/lib"}},startup:{ready(){const e=MathJax._.input.tex.Configuration.Configuration,t=MathJax._.input.tex.SymbolMap.CommandMap;new t("sfrac",{sfrac:"SFrac"},{SFrac(e,t){const n=e.ParseArg(t),s=e.ParseArg(t),o=e.create("node","mfrac",[n,s],{bevelled:!0});e.Push(o)}}),e.create("sfrac",{handler:{macro:["sfrac"]}}),MathJax.startup.defaultReady()}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js integrity crossorigin=anonymous></script></body></html>