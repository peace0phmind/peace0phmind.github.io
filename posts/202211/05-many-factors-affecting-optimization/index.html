<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/images/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/images/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/images/icons/apple_touch_icon_next.png><meta itemprop=name content="Many Factors Affecting Optimization"><meta itemprop=description content="05-many-factors-affecting-optimization"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://peace0phmind.github.io/images/avatar.png"><meta itemprop=keywords content="optimization"><meta property="og:type" content="article"><meta property="og:title" content="Many Factors Affecting Optimization"><meta property="og:description" content="05-many-factors-affecting-optimization"><meta property="og:image" content="/images/avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/"><meta property="og:site_name" content="Mind's Home"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="peace0phmind"><meta property="article:published_time" content="2022-11-20 11:22:00 +0800 +0800"><meta property="article:modified_time" content="2022-11-20 11:22:00 +0800 +0800"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.dabf51c13eb53dd19c627065c0fe53422155aaa07efc790ec29ea1d5b51707b8.css><style type=text/css>.post-footer{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){console.log("aaaa hello world!");const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"05-many-factors-affecting-optimization","permalink":"https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/","title":"Many Factors Affecting Optimization","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>Many Factors Affecting Optimization - Mind's Home</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class="header not-print" itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Mind's Home</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Notebook</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#机器学习的一般步骤>机器学习的一般步骤</a><ul><li><a href=#model-bias>Model Bias</a></li><li><a href=#optimization-issue>Optimization Issue</a></li><li><a href=#model-bias-vs-optimization-issue>Model Bias v.s. Optimization Issue</a><ul><li><a href=#optimization-issue-1>Optimization Issue</a></li></ul></li><li><a href=#overfitting>Overfitting</a></li><li><a href=#bias-complexity-trade-off>Bias-Complexity Trade-off</a></li><li><a href=#n-fold-cross-validation>N-fold Cross Validation</a></li><li><a href=#mismatch>Mismatch</a></li></ul></li><li><a href=#optimization-fails-because>Optimization Fails because</a><ul><li><a href=#tayler-series-approximation泰勒级数逼近>Tayler Series Approximation(泰勒级数逼近)</a></li><li><a href=#hessian>Hessian</a></li><li><a href=#saddle-point-vs-local-minima>Saddle Point v.s. Local Minima</a><ul><li><a href=#minimum-ratio>Minimum Ratio</a></li></ul></li></ul></li><li><a href=#batch>Batch</a><ul><li><a href=#small-batch-vs-large-batch>Small Batch v.s. Large Batch</a><ul><li><a href=#由于有gpu的平行运算的能力从性能的角度出发得到如下结论>由于有GPU的平行运算的能力，从性能的角度出发得到如下结论：</a></li><li><a href=#从准确率来看>从准确率来看</a></li><li><a href=#兼顾速度与generalization的研究文章>兼顾速度与Generalization的研究文章</a></li></ul></li></ul></li><li><a href=#momentum>Momentum</a><ul><li><a href=#vanilla-gradient-descent>(Vanilla) Gradient Descent</a></li><li><a href=#gradient-descent--momentum>Gradient Descent + Momentum</a></li></ul></li><li><a href=#adaptive-learning-rate>Adaptive Learning Rate</a><ul><li><a href=#root-mean-square-used-in-adagrad>Root Mean Square (Used in Adagrad)</a></li><li><a href=#learning-rate-adapts-dynamically>Learning rate adapts dynamically</a><ul><li><a href=#rmsprop>RMSProp</a></li><li><a href=#adam-rmsprop--momentum>Adam: RMSProp + Momentum</a></li></ul></li><li><a href=#learning-rate-scheduling>Learning Rate Scheduling</a></li></ul></li><li><a href=#summary-of-optimization>Summary Of Optimization</a></li><li><a href=#loss-for-classification>Loss for Classification</a><ul><li><a href=#class-as-one-hot-vector>Class as one-hot vector</a></li><li><a href=#soft-max>Soft-max</a></li><li><a href=#loss-function>Loss function</a></li><li><a href=#loss-function-affect-optimization>Loss function affect Optimization</a></li></ul></li><li><a href=#normalization>Normalization</a><ul><li><a href=#changing-landscape>Changing Landscape</a></li><li><a href=#feature-normalization>Feature Normalization</a><ul><li><a href=#theta的normalization>$\theta$的Normalization</a></li><li><a href=#feature-normalization的计算>Feature Normalization的计算</a></li><li><a href=#batch-normalization>Batch normalization</a></li><li><a href=#batch-normalization---testing>Batch normalization - Testing</a></li><li><a href=#batch-normalization-refs>Batch Normalization Refs</a></li></ul></li><li><a href=#normalization-refs>Normalization Refs</a></li></ul></li><li><a href=#new-optimization>New Optimization</a><ul><li><a href=#what-you-have-known-before>What you have known before?</a><ul><li><a href=#some-notations>Some Notations</a></li><li><a href=#what-is-optimization-about->What is Optimization about ?</a></li><li><a href=#on-line-vs-off-line-learning>On-line vs Off-line learning</a></li><li><a href=#sgd-stochastic-gradient-descent>SGD (stochastic gradient descent)</a></li><li><a href=#sgdm-stochastic-gradient-descent-with-momentum>SGDM (stochastic gradient descent with momentum)</a></li><li><a href=#adagrad>Adagrad</a></li><li><a href=#rmsprop-1>RMSProp</a></li><li><a href=#adam>Adam</a></li></ul></li><li><a href=#optimizers-real-application>Optimizers: Real Application</a></li><li><a href=#adam-vs-sgdm>Adam vs SGDM</a><ul><li><a href=#simply-combine-adam-with-sgdm>Simply combine Adam with SGDM?</a></li><li><a href=#towards-improving-adam>Towards improving Adam</a></li><li><a href=#towards-improving-sgdm>Towards Improving SGDM</a></li><li><a href=#learning-rate-range-test>Learning Rate range test</a></li></ul></li><li><a href=#does-adam-need-warm-up>Does Adam need warm-up?</a><ul><li><a href=#radam>RAdam</a></li></ul></li><li><a href=#radam-vs-swats>RAdam vs SWATS</a></li><li><a href=#k-step-forward-1-step-back>K step forward, 1 step back</a></li></ul></li><li><a href=#reference-video>Reference Video</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=peace0phmind src=/images/img-lazy-loading.gif data-src=/images/avatar.png><p class=site-author-name itemprop=name>peace0phmind</p><div class=site-description itemprop=description></div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/posts/><span class=site-state-item-count>32</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>2</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>13</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/peace0phmind title="Github → https://github.com/peace0phmind" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2020-10-20T19:37:14+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=29341></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=77></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-11-29T14:36:41+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class="tool-buttons not-print"><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a>
<a href=https://github.com/peace0phmind rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/images/avatar.png"><meta itemprop=name content="peace0phmind"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="peace0phmind"><meta itemprop=description content></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="Many Factors Affecting Optimization"><meta itemprop=description content="05-many-factors-affecting-optimization"></span><header class="post-header not-print"><h1 class=post-title itemprop="name headline">Many Factors Affecting Optimization
<a href=https://github.com/peace0phmind/peace0phmind.github.io/tree/master/content/posts/202211/05-many-factors-affecting-optimization.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-11-20 11:22:00 +0800 +0800" itemprop="dateCreated datePublished" datetime="2022-11-20 11:22:00 +0800 +0800">2022-11-20</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/ itemprop=url rel=index><span itemprop=name></span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>6216</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>13分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/posts/202211/05-many-factors-affecting-optimization/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body print autonumber" itemprop=articleBody><h2 id=机器学习的一般步骤>机器学习的一般步骤</h2><div class=markmap style=height:200px><script type=text/template>---
markmap:
  maxWidth: 300
  colorFreezeLevel: 6
  initialExpandLevel: 10
---

# loss on training data

## large
- [model bias](#model-bias)
  - make your model complex
- [optimization](#optimization-issue)

## small
- loss on testing data
  - large
    - [overfitting](#overfitting)
      - more training data
      - data augmentation
      - make your model simpler
    - [mismatch](#mismatch)
  - small 😊
  
## [model complex trade-off(权衡)](#bias-complexity-trade-off)
- Split your training data into training set and validation set for model selection</script></div><h3 id=model-bias>Model Bias</h3><ul><li>The model is too simple.<ul><li>find a needle in a haystack (大海捞针)</li><li>but there is no needle</li></ul></li><li>Solution: redesign your model to make it more flexible<ul><li>more features</li><li>more neurons, layers</li></ul></li></ul><h3 id=optimization-issue>Optimization Issue</h3><ul><li>Large loss not always imply model bias. There is another possibility &mldr;<ul><li>A needle is in a haystack&mldr;, Just cannot find it.</li></ul></li></ul><h3 id=model-bias-vs-optimization-issue>Model Bias v.s. Optimization Issue</h3><ul><li>Gaining the insights from comparison</li><li>当在测试数据上和训练数据上有着类似的loss曲线时，这说明是<code>Optimization Issue</code>的问题</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.009.jpg alt></figure></p><h4 id=optimization-issue-1>Optimization Issue</h4><ul><li>Start from shallower networks(or other models), which are easier to optimize.</li><li>从更容易优化的较浅的网络（或其他模型）开始。</li><li>If deeper networks do not obtain smaller loss on <code>training data</code>, then there is optimization issue.</li><li>如果更深的网络在“训练数据”上没有获得更小的损失，那么就存在优化问题。</li></ul><h3 id=overfitting>Overfitting</h3><ul><li>Small loss on training data, large loss on testing data. Why?</li><li>数据分布的这条虚线通常是无法明确的获知的，我们通常只能拿到在这条曲线上的多个<code>Training Data</code></li><li>由于model的Flexible, 训练出来的这个模型，在没有训练数据的地方会有“freestyle”, 从而导致测试数据的overfitting<ul><li>增加训练数据</li><li>Data augmentation(用一些对这个问题的理解，自己创造出新的训练数据。例如：对图片左右反转，或者是截取其中一块等)</li><li>通过限制model来解决overfitting，给model制造限制的方法：<ul><li>make your model simpler</li><li>Less parameters, sharing parameters<ul><li>Fully-connected的架构是一个比较有弹性的架构；而CNN是一个比较有限制的架构（根据影像的特性来限制模型的弹性）</li></ul></li><li>Less features</li><li>Early stopping</li><li>Regularization</li><li>Dropout</li></ul></li><li>这里需要注意，太多的限制和太简单的模型会导致<code>model bias</code></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.014.jpg alt></figure></p><h3 id=bias-complexity-trade-off>Bias-Complexity Trade-off</h3><ul><li>通过观察<code>Training loss</code>和<code>Testing loss</code>的loss曲线来选择model和对应的模型限制</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.020.jpg alt></figure></p><h3 id=n-fold-cross-validation>N-fold Cross Validation</h3><ul><li>Cross Validation就是N-flod Cross Validation的一个特例</li><li>如果使用<code>Cross Validation</code>, 则使用<code>Validation Set</code>的loss最小进行模型的选择</li><li>当使用<code>N-fold Cross Validation</code>时，则使用mse的avg最小来挑选模型</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.024.jpg alt></figure></p><h3 id=mismatch>Mismatch</h3><ul><li>Your training and testing data have different distributions.</li><li>需要对训练资料和测试资料有一定的了解才能分清到底是不是mismatch</li><li>mismatch和overfitting不是一个东西，overfitting可以通过增加训练资料来解决，而mismatch无法通过增加训练资料来解决</li></ul><h2 id=optimization-fails-because>Optimization Fails because</h2><ul><li>loss is <code>Not small enough</code>, because the gradient is close to zero.</li><li>Gradient为零的情况有：<code>local minima</code>, <code>local maxima</code>, <code>saddle point</code>等</li><li><code>saddle point</code>: Gradient为零, 同时既不是<code>local minima</code>也不是<code>local maxima</code>的地方</li><li>Gradient为零的点统称为<code>critical point</code></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.002.jpg alt></figure></p><h3 id=tayler-series-approximation泰勒级数逼近>Tayler Series Approximation(泰勒级数逼近)</h3><ul><li>如何知道一个<code>critical point</code>是<code>local minima</code>还是<code>saddle point</code></li><li>其中包括 Gradient $\color{green}g$ is a <u>vector</u>, Hessian $\color{red}H$ is a <u>matrix</u>.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.004.jpg alt></figure></p><h3 id=hessian>Hessian</h3><ul><li>Gradient $\color{green}g$ 为0时，则可知目前所在位置为临界点<code>Critical Point</code></li><li>Hessian $\color{red}H$ can telling the properties of critical points.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.005.jpg alt></figure></p><ul><li>当$\color{red}H$这个矩阵中的值全部为正值，则当前所在为<code>Local Minima</code></li><li>当$\color{red}H$这个矩阵中的值全部为负值，则当前所在为<code>Local Maxima</code></li><li>当$\color{red}H$这个矩阵中的值有正有负，则当前所在为<code>Saddle Point</code></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.006.jpg alt></figure></p><h3 id=saddle-point-vs-local-minima>Saddle Point v.s. Local Minima</h3><ul><li>在一维的空间中看到的local minima，在二维的空间中看到的可能就只是saddle point.</li><li>当我们有更多的参数，也许local minima是很少见的</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.014.jpg alt></figure></p><h4 id=minimum-ratio>Minimum Ratio</h4><ul><li>是所有<code>Local Minima</code>的数量与所有<code>Critical Point</code>的比值</li><li>从图上可知，最大的ratio也只是0.6</li><li>图上Eigen Values就是前文所说的Hessian Matrix.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.015.jpg alt></figure></p><h2 id=batch>Batch</h2><ul><li>不会拿所有的资料去算微分，会把所有的资料分成很多个batch，</li><li>每个batch的资料算一个loss，算一个Gradient再update参数</li><li>所有的资料算过一遍叫做一个epoch</li><li><code>shaffle</code> after each epoch, <code>shaffle</code>有很多不同的做法：<ul><li>一个常见的做法是在每个epoch开始之前，会分一次batch，每一个epoch的batch都不一样</li></ul></li></ul><h3 id=small-batch-vs-large-batch>Small Batch v.s. Large Batch</h3><ul><li>Consider we have 20 examples(N=20)</li><li>Batch size = N (Full batch)<ul><li>Update after seeing all the 20 examples</li><li>Long time for cooldown but powerful</li></ul></li><li>Batch size = 1<ul><li>Update for each example, Update 20 times in an epoch</li><li>Short time for cooldown but noisy</li></ul></li></ul><h4 id=由于有gpu的平行运算的能力从性能的角度出发得到如下结论>由于有GPU的平行运算的能力，从性能的角度出发得到如下结论：</h4><ul><li>Larger batch size does not require longer time to compute gradient(unless batch size is too large)</li><li>Smaller batch requires longer time for one epoch (longer time for seeing all data once)</li></ul><h4 id=从准确率来看>从准确率来看</h4><ul><li>反而是有noisy的batch可以得到好的结果。Smaller batch size has better performance.</li><li>如下图，横轴是batch size，纵轴是正确率。如图可知batch size越大，validation set上的结果越差。</li><li>这个是overfitting么？这个不是overfitting, 因为我们用的数据和模型都是一致的。所以这里发生在larger batch size上的情况是Optimization Fails.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.008.jpg alt></figure></p><p>为什么在Noisy的batch size上update更好呢？一种可能的解释是：</p><ul><li>Full Batch比较容易stuck，而Small Batch由于不同batch的数据有所不同，所以相对来说不太容易stuck，更容易train到比较小的loss</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.009.jpg alt></figure></p><p>有研究表明，小的batch size不仅针对training有效，在testing的时候也比大的batch size要好。如下图：</p><ul><li>数据相同，模型相同的情况下，将大的batch size在training set上的accuracy调整的和小的batch size一样</li><li>而从图上右侧表格观察，LB的accuracy比SB的accuracy要差，这是overfitting</li><li>详见资料：
<a href=https://arxiv.org/abs/1609.04836 title="On Large-Batch Training for deep Learning: Generalization Gap and Sharp Minima" rel="noopener external nofollow noreferrer" target=_blank class=exturl>On Large-Batch Training for deep Learning: Generalization Gap and Sharp Minima
<i class="fa fa-external-link-alt"></i></a></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.010.jpg alt></figure></p><p>为什么会有这种现象呢？</p><ul><li>假设如下图的training loss上有很多个Local Minima，这些Local Minima的Loss都足够小</li><li>但是Local Minima还是有好坏之分的。如图中，Flat minima（盆地）的容错性要优于sharp minima（峡谷）。</li><li>大的batch size倾向于走到峡谷里面，而小的batch size倾向于走到盆地里面。</li><li>小的batch size有很多的noisy，它每次走的方向都不太一样，如果这个峡谷比较的窄，那么noisy的batch size很容易跳出峡谷。</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.011.jpg alt></figure></p><table><thead><tr><th> </th><th>Small</th><th>Large</th></tr></thead><tbody><tr><td>Speed for one update (no parallel)</td><td>Faster</td><td>Slower</td></tr><tr><td>speed for one update (with parallel)</td><td>Same</td><td>Same(not too large)</td></tr><tr><td>Time for one epoch</td><td>Slower</td><td>Faster</td></tr><tr><td>Gradient</td><td>Noisy</td><td>Stable</td></tr><tr><td>Optimization</td><td>Better</td><td>Worse</td></tr><tr><td>Generalization</td><td>Better</td><td>Worse</td></tr></tbody></table><span style=color:red>Batch size is a hyperparameter you have to decide.</span><h4 id=兼顾速度与generalization的研究文章>兼顾速度与Generalization的研究文章</h4><ul><li><a href=https://arxiv.org/abs/1904.00962 title="Large Batch Optimization for Deep Learning: Training BERT in 76 minutes" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1711.04325 title="Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/2001.02312 title="Stochastic Weight Averaging in Parallel: Large-Batch Training that Generalizes Well" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Stochastic Weight Averaging in Parallel: Large-Batch Training that Generalizes Well
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1708.03888 title="Large Batch Training of Convolutional Networks" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Large Batch Training of Convolutional Networks
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1706.02677 title="Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour
<i class="fa fa-external-link-alt"></i></a></li></ul><h2 id=momentum>Momentum</h2><h3 id=vanilla-gradient-descent>(Vanilla) Gradient Descent</h3><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.016.jpg alt></figure></p><h3 id=gradient-descent--momentum>Gradient Descent + Momentum</h3><ul><li>Movement: movement of last step minus gradient at present</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.017.jpg alt></figure></p><h2 id=adaptive-learning-rate>Adaptive Learning Rate</h2><ul><li>$ \text{Training stuck} \ne \text{Small Gradient} $</li><li>当loss不再下降的时候，需要确认一下Gradient是否为0；即loss不再下降需要分析stuck的原因</li><li>如图，当loss不再下降时，norm of gradient 并没有为0</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.002.jpg alt></figure></p><p>Training can be difficult even without critical points.</p><ul><li>Learning rate cannot be one-size-fits-all(一刀切).</li><li>Different parameters needs different learning rate.</li><li>相对平坦的Gradient Descent需要较大的Learning Rate</li><li>相对尖锐的Gradient Descent需要较小的Learning Rate</li></ul><p>Formulation for <code>one</code> parameter:</p><p>\begin{align}
\theta_i^{t+1} & \leftarrow \theta_i^t - {\color{red}\eta}g_i^t \cr
g_i^t & = \frac{\partial L}{\partial \theta_i} |_{\theta=\theta^t} \cr
& \Downarrow \cr
\theta_i^{t+1} & \leftarrow \theta_i^t - {\color{red}\frac{\eta}{\sigma_i^t}}g_i^t
\end{align}</p><p>${\color{red}\frac{\eta}{\sigma_i^t}}$就是<code>Parameter dependent</code>的Learning Rate,下面介绍几种常见的计算方法：</p><h3 id=root-mean-square-used-in-adagrad>Root Mean Square (Used in Adagrad)</h3><p>\begin{align}
\theta_i^1 & \leftarrow \theta_i^0 - \frac{\eta}{\sigma_i^0}g_i^0 & \sigma_i^0 &= \sqrt{(g_i^0)^2} = |g_i^0| \cr
\theta_i^2 & \leftarrow \theta_i^1 - \frac{\eta}{\sigma_i^1}g_i^1 & \sigma_i^1 &= \sqrt{\frac{1}{2}[(g_i^0)^2+(g_i^1)^2]} \cr
\theta_i^3 & \leftarrow \theta_i^2 - \frac{\eta}{\sigma_i^2}g_i^2 & \sigma_i^2 &= \sqrt{\frac{1}{3}[(g_i^0)^2+(g_i^1)^2+(g_i^2)^2]} \cr
& \vdots \cr
\theta_i^{t+1} & \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t & \sigma_i^t &= \sqrt{\frac{1}{t+1}\sum_{i=0}^t(g_i^t)^2}
\end{align}</p><ul><li>小的$\sigma_i^t$会有大的step</li><li>大的$\sigma_i^t$会有小的step</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.007.jpg alt></figure></p><h3 id=learning-rate-adapts-dynamically>Learning rate adapts dynamically</h3><ul><li>即使针对同一个参数，在不同的时候，可能也需要有不同的Learning Rate</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.008.jpg alt></figure></p><h4 id=rmsprop>RMSProp</h4><p>\begin{align}
\theta_i^1 & \leftarrow \theta_i^0 - \frac{\eta}{\sigma_i^0}g_i^0 & \sigma_i^0 &= \sqrt{(g_i^0)^2} = |g_i^0| \cr
& & & \text{设 } 0 &lt; \alpha &lt; 1 \cr
\theta_i^2 & \leftarrow \theta_i^1 - \frac{\eta}{\sigma_i^1}g_i^1 & \sigma_i^1 &= \sqrt{\alpha(\sigma_i^0)^2 + (1-\alpha)(g_i^1)^2} \cr
\theta_i^3 & \leftarrow \theta_i^2 - \frac{\eta}{\sigma_i^2}g_i^2 & \sigma_i^2 &= \sqrt{\alpha(\sigma_i^1)^2 + (1-\alpha)(g_i^2)^2]} \cr
& \vdots \cr
\theta_i^{t+1} & \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t & \sigma_i^t &= \sqrt{\alpha(\sigma_i^{t-1})^2 + (1-\alpha)(g_i^t)^2}
\end{align}</p><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.010.jpg alt></figure></p><h4 id=adam-rmsprop--momentum>Adam: RMSProp + Momentum</h4><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.011.jpg alt></figure></p><h3 id=learning-rate-scheduling>Learning Rate Scheduling</h3><ul><li>Learning Rate Decay<ul><li>After the training goes, we are closer to the destination, so we reduce the learning rate.</li></ul></li><li>Warm Up<ul><li>Increase and then decrease?</li><li><a href=https://arxiv.org/abs/1512.03385 title="Deep Residual Learning for Image Recognition" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Deep Residual Learning for Image Recognition
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Attention Is All You Need
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1908.03265 title="On the Variance of the Adaptive Learning Rate and Beyond" rel="noopener external nofollow noreferrer" target=_blank class=exturl>On the Variance of the Adaptive Learning Rate and Beyond
<i class="fa fa-external-link-alt"></i></a></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.016.jpg alt></figure></p><h2 id=summary-of-optimization>Summary Of Optimization</h2><ul><li>Momentum: weighted sum of the previous gradients (考虑方向)</li><li>$\sigma_i^t$: 只考虑大小不考虑方向</li><li>$\eta^t$: Learning rate scheduling</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.017.jpg alt></figure></p><h2 id=loss-for-classification>Loss for Classification</h2><h3 id=class-as-one-hot-vector>Class as one-hot vector</h3><ul><li>one-hot vector for multi-output</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.005.jpg alt></figure></p><h3 id=soft-max>Soft-max</h3><ul><li>soft-max对上层多输出结果做一个normalize</li><li>并且让大的值和小的值之间的差距更大</li><li>soft-max有时又叫做logit</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.007.jpg alt></figure></p><h3 id=loss-function>Loss function</h3><ul><li>Mean Square Error (MSE): $ e = \sum\limits_i(\hat y_i - y_i^\prime)^2 $</li><li>Cross-entropy: $ e = -\sum\limits_i \hat y_i ln y_i^\prime $</li><li>Minimizing cross-entropy is equivalent to maximizing likelihood.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.008.jpg alt></figure></p><h3 id=loss-function-affect-optimization>Loss function affect Optimization</h3><ul><li>设目前做一个三分类的模型，当前这个分类的结果是$ \hat y = \begin{bmatrix}1 \cr 0 \cr 0 \end{bmatrix} $</li><li>e表示$y到\hat y$之间的距离，可以是MSE，也可以是Cross-entropy</li><li>$y_1$的取值范围为[-10, 10], $y_2$的取值范围为[-10, 10], $y_3$的取值为固定值-1000</li><li>下图左右分别为MSE和Cross-entropy想对于y的取值的Error Surface,</li><li>这两张图中Error Surface的特点都是右下角loss小，左上角loss大</li><li>假设我们开始的地方都是左上角：如果我们选择Cross-entropy，左上角的地方是有斜率的,所以可以通过gradient的方法一路向右下角走达到small loss;
如果我们选择MSE，我们就卡住了，在MSE左上角这个loss很大的地方，它的gradient非常小，趋进于零，而距离目标又很远，没有很好的办法通过gradient的方法走到右下角。</li><li>所以如果做classification时，选择使用MSE做loss时，有很大可能性train不起来; 当然如果使用类似Adam这些好的Optimizer时，也许有机会走到右下角。</li><li><span style=color:red>Changing the loss function can change the difficulty of optimization.</span></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.009.jpg alt></figure></p><h2 id=normalization>Normalization</h2><h3 id=changing-landscape>Changing Landscape</h3><ul><li>$w_1, w_2$与不同的feature相关，由于不同的feature范围不同，导致了$w_1, w_2$的变动对最终的loss产生不同的影响</li><li>是否可以找到一个方法，让不同的feature有着相似的range</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.003.jpg alt></figure></p><h3 id=feature-normalization>Feature Normalization</h3><ul><li>下图只是Feature Normalization的一种可能</li><li>$ x^1, x^2, \cdots, x^R $ 为数据的R个features</li><li>For each dimension i:<ul><li>$m_i$ : mean</li><li>$\sigma_i$ : standard deviation</li></ul></li><li>Normalization: $\tilde{x}_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i} $<ul><li>The means of all dims are 0, and the variances are all 1</li></ul></li><li>In general, feature normalization makes gradient descent converge faster.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.004.jpg alt></figure></p><h4 id=theta的normalization>$\theta$的Normalization</h4><ul><li>$\tilde{x}^1$在经过$W^1$后得到的$z^1$也是具有不同的range的，这将导致针对$W^2$的optimize会比较的困难</li><li>对于$W^2$来说，这里的z或者a也是feature，所以这里需要考虑对z或者a做Normalization</li><li>那么到底在激活函数的前面还是后面做normalization呢？实作中都可以，但当activation function为Sigmoid时，建议在Sigmoid的前面，也就是这里的z做normalization。</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.005.jpg alt></figure></p><h4 id=feature-normalization的计算>Feature Normalization的计算</h4><ul><li>$\mu = \frac{1}{n}\sum\limits_{i=1}^nz^i $</li><li>$ \sigma = \sqrt{\frac{1}{n}\sum\limits_{i=1}^n(z^i-\mu)^2} $</li><li>$ \tilde{z}^i = \frac{z^i-\mu}{\sigma} $</li></ul><h4 id=batch-normalization>Batch normalization</h4><ul><li>由于需要对$x, z^1, z^2, \cdots, z^n$都做normalization，所以这是一个巨大的network，如果使用这个巨大的network针对所有的数据去求$\mu和\sigma$是不太现实的，
所以只能将范围缩小到一个batch，从而诞生了batch normalization。</li><li>而在做Batch normalization的时候，往往还会加上一个$\beta和\gamma$， 得到方程$ \hat{z}^i = \gamma\odot\tilde{z}^i+\beta $ 其中 $ \tilde{z}^i = \frac{z^i-\mu}{\sigma} $</li><li>实际在做训练时，考虑到normalization的情况，在初始情况下$\gamma$会被初始化为one vector(全1的向量), 而$\beta$会被初始化为zero vector(全0的向量)。</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.008.jpg alt></figure></p><h4 id=batch-normalization---testing>Batch normalization - Testing</h4><ul><li>We do not always have batch at testing stage.</li><li>Computing the moving average of $\mu$ and $\sigma$ of the batches during training.</li><li>如下计算出$\bar{\mu}, \bar{\sigma}$，替换training中的表达式$ \tilde{z} = \frac{z-\mu}{\sigma} $得到$ \tilde{z} = \frac{z-\bar{\mu}}{\bar{\sigma}} $</li><li>How to compute moving average?</li></ul><p>\begin{align}
\mu^1, \mu^2, \mu^3, \cdots, \mu^t \cr
\bar{\mu} \leftarrow p\bar{\mu} + (1-p)\mu^t
\end{align}</p><h4 id=batch-normalization-refs>Batch Normalization Refs</h4><ul><li><a href=https://arxiv.org/abs/1502.03167 title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1805.11604 title="How Does Batch Normalization Help Optimization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>How Does Batch Normalization Help Optimization
<i class="fa fa-external-link-alt"></i></a><ul><li>Experimental results (and theoretically analysis) support batch normalization change the landscape of error surface.</li><li>This suggests that the positive impact of BatchNorm on training might be somewhat serendipitous(偶然的，发现了一个意料之外的东西).</li></ul></li></ul><h3 id=normalization-refs>Normalization Refs</h3><ul><li><a href=https://arxiv.org/abs/1702.03275 title="Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1607.06450 title="Layer Normalization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Layer Normalization
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1607.08022 title="Instance Normalization: The Missing Ingredient for Fast Stylization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Instance Normalization: The Missing Ingredient for Fast Stylization
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1803.08494 title="Group Normalization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Group Normalization
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1602.07868 title="Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1705.10941 title="Spectral Norm Regularization for Improving the Generalizability of Deep Learning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Spectral Norm Regularization for Improving the Generalizability of Deep Learning
<i class="fa fa-external-link-alt"></i></a></li></ul><h2 id=new-optimization>New Optimization</h2><h3 id=what-you-have-known-before>What you have known before?</h3><ul><li>SGD - stochastic gradient descent: 随机梯度下降</li><li>SGDM - stochastic gradient descent with momentum</li><li>Adagrad</li><li>RMSProp</li><li>Adam</li></ul><h4 id=some-notations>Some Notations</h4><ul><li>$\theta_t$: model parameters at time step t</li><li>$\nabla L(\theta_t) \text{or } g_t$: gradient at $\theta_t$, used to compute $\theta_{t+1}$</li><li>$m_{t+1}$: momentum accumulated from time step 0 to time step t, which is used to compute $\theta_{t+1}$</li></ul><p>\begin{align}
g_t \cr
\overleftarrow{x_t \rightarrow \theta_t \rightarrow y_t ^\underleftrightarrow{L(\theta_t;x_t)} \hat{y}_t}
\end{align}</p><h4 id=what-is-optimization-about->What is Optimization about ?</h4><ul><li>Find a $\theta$ to get the lowest $\sum_x L(\theta; x)$ !!</li><li>Or, Find a $\theta$ to get the lowest $L(\theta)$ !!</li></ul><h4 id=on-line-vs-off-line-learning>On-line vs Off-line learning</h4><ul><li>On-line: one pair of $(x_t, \hat{y}_t)$ at a time step</li><li>Off-line: pour all $(x_t, \hat{y}_t)$ into the model at every time step</li></ul><h4 id=sgd-stochastic-gradient-descent>SGD (stochastic gradient descent)</h4><ul><li>Start at position $\theta^0$</li><li>Compute gradient at $\theta^0$</li><li>Move to $\theta^1 = \theta^0 - \eta \nabla L(\theta^0)$</li><li>Compute gradient at $\theta^1$</li><li>Move to $\theta^2 = \theta^1 - \eta \nabla L(\theta^1)$</li><li>Stop until $\nabla L(\theta^t) \approx 0$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.010.jpg alt></figure></p><h4 id=sgdm-stochastic-gradient-descent-with-momentum>SGDM (stochastic gradient descent with momentum)</h4><ul><li>Start at point $\theta^0$</li><li>Movement $V^0=0$</li><li>Compute gradient at $\theta^0$</li><li>Movement $V^1=\lambda V^0 - \eta \nabla L(\theta^0)$</li><li>Move to $\theta^1 = \theta^0 + V^1$</li><li>Compute gradient at $\theta^1$</li><li>Movement $V^2 = \lambda V^1 - \eta \nabla L(\theta^1)$</li><li>Move to $\theta^2 = \theta^1 + V^2$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.011.jpg alt></figure></p><h4 id=adagrad>Adagrad</h4><ul><li>$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\sum_{i=0}^{t-1}(g_i)^2}}g_{t-1}$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.014.jpg alt></figure></p><h4 id=rmsprop-1>RMSProp</h4><ul><li>$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t}}g_{t-1}$</li><li>$v_1 = g_0^2$</li><li>$v_t = \alpha v_{t-1} + (1-\alpha)(g_{t-1})^2$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.015.jpg alt></figure></p><h4 id=adam>Adam</h4><ul><li>$ \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \varepsilon}\hat{m}_t $</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.016.jpg alt></figure></p><h3 id=optimizers-real-application>Optimizers: Real Application</h3><ul><li>BERT: 作Q/A， 文章生成， 2018年提出，使用 ADAM</li><li>Transformer: 用于翻译， 使用 ADAM</li><li>Tacotron: 最早使用神经网络作逼真的语音生成的模型, 2017年提出, 使用 ADAM</li><li>Yolo: 使用 SGDM</li><li>Mask R-CNN: 使用 SGDM</li><li>ResNet: 使用 SGDM</li><li>Gig-GAN: 生成影像， 使用 ADAM</li><li>MEMO: 在不同的分类任务中学到共同的资讯， 使用 ADAM</li></ul><h3 id=adam-vs-sgdm>Adam vs SGDM</h3><ul><li>Adam: fast training, large generalization gap, unstable</li><li>SGDM: stable, little generalization gap, better convergence(?)</li></ul><h4 id=simply-combine-adam-with-sgdm>Simply combine Adam with SGDM?</h4><ul><li>SWATS: Begin with Adam(fast), end with SGDM</li></ul><h4 id=towards-improving-adam>Towards improving Adam</h4><h5 id=troubleshooting>Troubleshooting</h5><ul><li>根据前面的计算公式，从100000到100998的梯度都是1,所以movement都是$\eta$</li><li>第100999步的梯度很大，100000,对应的movement为$10\sqrt{10}\eta$</li><li>这导致了前面很多无用的批次的梯度产生了约1000$\eta$的无用变化（乱走），而其中一个有用的很小的批次带来的变化只有33个$\eta$</li><li>当梯度大部分都很小时，就会产生这样的问题；有用的大的梯度对应的批次会被大量的小的无用的梯度牵着鼻子走</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.029.jpg alt></figure></p><h5 id=amsgrad>AMSGrad</h5><ul><li>Reduce the influence of non-informative gradients</li><li>Remove de-biasing due to the max operation</li><li>这个算法的改进可以类比为Adagrad和RMSProp, 所以感觉并没有起到很好的效果</li></ul><h5 id=troubleshooting-1>Troubleshooting</h5><ul><li>In the final stage of training, most gradients are small and non-informative, while some mini-batches provide large informative gradient rarely</li><li>Learning rates are either extremely large(for small gradients) or extremely small(for large gradients)</li></ul><h5 id=adabound>AdaBound</h5><ul><li>AMSGrad only handles large learning rates</li><li>AdaBound的公式中，有参数并非adaptive的，而是有点工程方法</li></ul><h4 id=towards-improving-sgdm>Towards Improving SGDM</h4><ul><li>Adaptive learning rate algorithms: dynamically adjust learning rate over time</li><li>SGD-type algorithms: fix learning rate for all updates&mldr; too slow for small learning rates and bad result for large learning rates</li><li>There might be a &ldquo;best&rdquo; learning rate?</li></ul><h4 id=learning-rate-range-test>Learning Rate range test</h4><ul><li>Learning Rate 在很大或很小的时候，性能都不会很好</li><li>Learning Rate 适中的时候，性能才比较好</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.035.jpg alt></figure></p><h5 id=cyclical-lr>Cyclical LR</h5><ul><li>learning rate: decide by LR range test</li><li>step size: several epochs</li><li>avoid local minimum by varying learning rate</li><li>learning rate在大小，大小的循环进行变化</li><li>变大的时候时在作exploration(探索)，变小的时候是在作收敛</li><li>The more exploration the better!</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.036.jpg alt></figure></p><h5 id=sgdr>SGDR</h5><ul><li>不用象Cyclical LR一样不断的变大再变小，而是在变小后，重新变回初始值重新开始</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.037.jpg alt></figure></p><h5 id=one-cycle-lr>One-Cycle LR</h5><ul><li>warm-up + annealing + fine-tuning</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.038.jpg alt></figure></p><h3 id=does-adam-need-warm-up>Does Adam need warm-up?</h3><ul><li>distorted(扭曲的) gradient -> distorted EMA squared gradients -> Bad learning rate</li><li>keep your step size small at the beginning of training helps to reduce the variance of the gradients</li><li>新的warm-up的方法是，先变小，再变大</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.040.jpg alt></figure></p><h4 id=radam>RAdam</h4><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.041.jpg alt></figure></p><h3 id=radam-vs-swats>RAdam vs SWATS</h3><table><thead><tr><th> </th><th>RAdam</th><th>SWATS</th></tr></thead><tbody><tr><td>Inspiration<br>灵感</td><td>Distortion of gradient at the beginning of training results in inaccurate adaptive learning rate<br>训练开始时梯度失真导致自适应学习率不准确</td><td>non-convergence and generalization gap of Adam, slow training of SGDM<br>Adam 的不收敛和泛化差距，SGDM 训练缓慢</td></tr><tr><td>How</td><td>Apply warm-up learning rate to reduce the influence of inaccurate adaptive learning rate<br>应用预热学习率减少自适应学习率不准确的影响</td><td>Combine their advantages by applying Adam first, then SGDM<br>通过先应用 Adam，然后应用 SGDM 来结合它们的优势</td></tr><tr><td>Switch</td><td>SGDM to RAdam</td><td>Adam to SGDM</td></tr><tr><td>Why switch</td><td>The approximation of the variance of $\hat{v}_t$ is invalid at the beginning of training<br>方差的近似值$\hat{v}_t$在训练开始时无效</td><td>To purse better convergence<br>追求更好的收敛</td></tr><tr><td>Switch point</td><td>When the approximation becomes valid<br>当近似值成立时</td><td>Some human-defined criteria<br>一些人为定义的标准</td></tr></tbody></table><h3 id=k-step-forward-1-step-back>K step forward, 1 step back</h3><ul><li>Lookahead: universal wrapper for all optimizers</li><li>这个算法有两组weight：<ul><li>这里$\theta$是用于explore(探索)的，叫做Fast weights</li><li>$\phi$是真正需要的weight，叫做Slow weights</li></ul></li><li>循环分外循环和内循环。<ul><li>内循环会走k步，内循环的Optim可以使用任意的optimizer</li><li>每走完一遍内循环，根据当前位置，到内循环开始前的位置，根据$\alpha$计算出下次循环开始的位置</li><li>接下来，使用新计算出来的$\phi$进行新一轮的内循环</li></ul></li><li>这个方法和Memo里面的演算法Reptile很像</li></ul><p>\begin{align}
& \text{For }t = 1, 2, \dots \text{(outer loop)} \cr
&  \theta_{t,0} = \phi_{t-1} \cr
&  \text{For } i = 1, 2, \dots, k \text{(inner loop)} \cr
&    \theta_{t,i} = \theta_{t, i-1} + \text{Optim(Loss, data, }\theta_{t, i-1}\text{)} \cr
&  \phi_t = \phi_{t-1} + \alpha(\theta_{t,k} - \phi_{t-1})
\end{align}</p><ul><li>1 step back: avoid too dangerous exploration</li><li>Look for a more flatten minimum</li><li>More stable</li><li>Better generalization</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.044.jpg alt></figure></p><h2 id=reference-video>Reference Video</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/WeHM2xpYQpw style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/QW6uINn7uGk style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/zzbr1h9sF54 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/HYUXEeh3kwY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/O2VkP8dJ5FE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/BABPWOkSbLE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/1_HBTJyWgNA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/4pUmZ8hXlHM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/e03YKGHXnL8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></div><footer class="post-footer not-print"><div class=post-tags><a href=/tags/optimization>optimization</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/202211/docker-swarm/ rel=next title="Docker Swarm"><i class="fa fa-chevron-left"></i> Docker Swarm</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/202211/04-logistic-regression/ rel=prev title="Logistic Regression">Logistic Regression
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class="footer not-print"><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>peace0phmind</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"https://peace0phmind.github.io","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.0","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.c000895f35c4f549795bbb87a678888fa0246f9513d456814f0988441a9a4b7c.js defer></script>
<script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin=anonymous></script>
<script type=text/javascript>mediumZoom("[data-zoomable]",{background:null})</script><script type=text/javascript>window.MathJax={options:{ignoreHtmlClass:"markmap"},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!1,packages:{"[+]":["base","extpfeil","ams","amscd","newcommand","knowl","sfrac"]}},chtml:{},loader:{load:["input/asciimath","[tex]/extpfeil","[tex]/amscd","[tex]/newcommand","[pretext]/mathjaxknowl3.js"],paths:{pretext:"https://pretextbook.org/js/lib"}},startup:{ready(){const e=MathJax._.input.tex.Configuration.Configuration,t=MathJax._.input.tex.SymbolMap.CommandMap;new t("sfrac",{sfrac:"SFrac"},{SFrac(e,t){const n=e.ParseArg(t),s=e.ParseArg(t),o=e.create("node","mfrac",[n,s],{bevelled:!0});e.Push(o)}}),e.create("sfrac",{handler:{macro:["sfrac"]}}),MathJax.startup.defaultReady()}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js integrity crossorigin=anonymous></script><style>.markmap>svg{width:100%;height:100%}</style><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader@0.14.4-alpha.1 integrity crossorigin=anonymous d></script></body></html>