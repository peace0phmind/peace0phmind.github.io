<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/images/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/images/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/images/icons/apple_touch_icon_next.png><meta itemprop=name content="Many Factors Affecting Optimization"><meta itemprop=description content="05-many-factors-affecting-optimization"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://peace0phmind.github.io/images/avatar.png"><meta itemprop=keywords content="optimization"><meta property="og:type" content="article"><meta property="og:title" content="Many Factors Affecting Optimization"><meta property="og:description" content="05-many-factors-affecting-optimization"><meta property="og:image" content="/images/avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/"><meta property="og:site_name" content="Mind's Home"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="peace0phmind"><meta property="article:published_time" content="2022-11-20 11:22:00 +0800 +0800"><meta property="article:modified_time" content="2022-11-20 11:22:00 +0800 +0800"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.dabf51c13eb53dd19c627065c0fe53422155aaa07efc790ec29ea1d5b51707b8.css><style type=text/css>.post-footer{content:"~ æˆ‘å¯æ˜¯æœ‰åº•çº¿çš„å“Ÿ ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){console.log("aaaa hello world!");const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"05-many-factors-affecting-optimization","permalink":"https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/","title":"Many Factors Affecting Optimization","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>Many Factors Affecting Optimization - Mind's Home</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class="header not-print" itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=åˆ‡æ¢å¯¼èˆªæ  role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Mind's Home</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Notebook</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>å…³äº</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>æœç´¢</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=æœç´¢... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>æ–‡ç« ç›®å½•</li><li class=sidebar-nav-overview>ç«™ç‚¹æ¦‚è§ˆ</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#æœºå™¨å­¦ä¹ çš„ä¸€èˆ¬æ­¥éª¤>æœºå™¨å­¦ä¹ çš„ä¸€èˆ¬æ­¥éª¤</a><ul><li><a href=#model-bias>Model Bias</a></li><li><a href=#optimization-issue>Optimization Issue</a></li><li><a href=#model-bias-vs-optimization-issue>Model Bias v.s. Optimization Issue</a><ul><li><a href=#optimization-issue-1>Optimization Issue</a></li></ul></li><li><a href=#overfitting>Overfitting</a></li><li><a href=#bias-complexity-trade-off>Bias-Complexity Trade-off</a></li><li><a href=#n-fold-cross-validation>N-fold Cross Validation</a></li><li><a href=#mismatch>Mismatch</a></li></ul></li><li><a href=#optimization-fails-because>Optimization Fails because</a><ul><li><a href=#tayler-series-approximationæ³°å‹’çº§æ•°é€¼è¿‘>Tayler Series Approximation(æ³°å‹’çº§æ•°é€¼è¿‘)</a></li><li><a href=#hessian>Hessian</a></li><li><a href=#saddle-point-vs-local-minima>Saddle Point v.s. Local Minima</a><ul><li><a href=#minimum-ratio>Minimum Ratio</a></li></ul></li></ul></li><li><a href=#batch>Batch</a><ul><li><a href=#small-batch-vs-large-batch>Small Batch v.s. Large Batch</a><ul><li><a href=#ç”±äºæœ‰gpuçš„å¹³è¡Œè¿ç®—çš„èƒ½åŠ›ä»æ€§èƒ½çš„è§’åº¦å‡ºå‘å¾—åˆ°å¦‚ä¸‹ç»“è®º>ç”±äºæœ‰GPUçš„å¹³è¡Œè¿ç®—çš„èƒ½åŠ›ï¼Œä»æ€§èƒ½çš„è§’åº¦å‡ºå‘å¾—åˆ°å¦‚ä¸‹ç»“è®ºï¼š</a></li><li><a href=#ä»å‡†ç¡®ç‡æ¥çœ‹>ä»å‡†ç¡®ç‡æ¥çœ‹</a></li><li><a href=#å…¼é¡¾é€Ÿåº¦ä¸generalizationçš„ç ”ç©¶æ–‡ç« >å…¼é¡¾é€Ÿåº¦ä¸Generalizationçš„ç ”ç©¶æ–‡ç« </a></li></ul></li></ul></li><li><a href=#momentum>Momentum</a><ul><li><a href=#vanilla-gradient-descent>(Vanilla) Gradient Descent</a></li><li><a href=#gradient-descent--momentum>Gradient Descent + Momentum</a></li></ul></li><li><a href=#adaptive-learning-rate>Adaptive Learning Rate</a><ul><li><a href=#root-mean-square-used-in-adagrad>Root Mean Square (Used in Adagrad)</a></li><li><a href=#learning-rate-adapts-dynamically>Learning rate adapts dynamically</a><ul><li><a href=#rmsprop>RMSProp</a></li><li><a href=#adam-rmsprop--momentum>Adam: RMSProp + Momentum</a></li></ul></li><li><a href=#learning-rate-scheduling>Learning Rate Scheduling</a></li></ul></li><li><a href=#summary-of-optimization>Summary Of Optimization</a></li><li><a href=#loss-for-classification>Loss for Classification</a><ul><li><a href=#class-as-one-hot-vector>Class as one-hot vector</a></li><li><a href=#soft-max>Soft-max</a></li><li><a href=#loss-function>Loss function</a></li><li><a href=#loss-function-affect-optimization>Loss function affect Optimization</a></li></ul></li><li><a href=#normalization>Normalization</a><ul><li><a href=#changing-landscape>Changing Landscape</a></li><li><a href=#feature-normalization>Feature Normalization</a><ul><li><a href=#thetaçš„normalization>$\theta$çš„Normalization</a></li><li><a href=#feature-normalizationçš„è®¡ç®—>Feature Normalizationçš„è®¡ç®—</a></li><li><a href=#batch-normalization>Batch normalization</a></li><li><a href=#batch-normalization---testing>Batch normalization - Testing</a></li><li><a href=#batch-normalization-refs>Batch Normalization Refs</a></li></ul></li><li><a href=#normalization-refs>Normalization Refs</a></li></ul></li><li><a href=#new-optimization>New Optimization</a><ul><li><a href=#what-you-have-known-before>What you have known before?</a><ul><li><a href=#some-notations>Some Notations</a></li><li><a href=#what-is-optimization-about->What is Optimization about ?</a></li><li><a href=#on-line-vs-off-line-learning>On-line vs Off-line learning</a></li><li><a href=#sgd-stochastic-gradient-descent>SGD (stochastic gradient descent)</a></li><li><a href=#sgdm-stochastic-gradient-descent-with-momentum>SGDM (stochastic gradient descent with momentum)</a></li><li><a href=#adagrad>Adagrad</a></li><li><a href=#rmsprop-1>RMSProp</a></li><li><a href=#adam>Adam</a></li></ul></li><li><a href=#optimizers-real-application>Optimizers: Real Application</a></li><li><a href=#adam-vs-sgdm>Adam vs SGDM</a><ul><li><a href=#simply-combine-adam-with-sgdm>Simply combine Adam with SGDM?</a></li><li><a href=#towards-improving-adam>Towards improving Adam</a></li><li><a href=#towards-improving-sgdm>Towards Improving SGDM</a></li><li><a href=#learning-rate-range-test>Learning Rate range test</a></li></ul></li><li><a href=#does-adam-need-warm-up>Does Adam need warm-up?</a><ul><li><a href=#radam>RAdam</a></li></ul></li><li><a href=#radam-vs-swats>RAdam vs SWATS</a></li><li><a href=#k-step-forward-1-step-back>K step forward, 1 step back</a></li></ul></li><li><a href=#reference-video>Reference Video</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=peace0phmind src=/images/img-lazy-loading.gif data-src=/images/avatar.png><p class=site-author-name itemprop=name>peace0phmind</p><div class=site-description itemprop=description></div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/posts/><span class=site-state-item-count>32</span>
<span class=site-state-item-name>æ—¥å¿—</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>2</span>
<span class=site-state-item-name>åˆ†ç±»</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>13</span>
<span class=site-state-item-name>æ ‡ç­¾</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/peace0phmind title="Github â†’ https://github.com/peace0phmind" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>ç½‘ç«™èµ„è®¯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>å·²è¿è¡Œï¼š</div><div class=item-count id=runTimes data-publishdate=2020-10-20T19:37:14+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>æ€»è®¿å®¢æ•°ï¼š</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>é¡µé¢æµè§ˆï¼š</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>æ€»å­—æ•°ï¼š</div><div class=item-count id=wordsCount data-count=29341></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>é˜…è¯»çº¦ï¼š</div><div class=item-count id=readTimes data-times=77></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>æœ€åæ›´æ–°äºï¼š</div><div class=item-count id=last-push-date data-lastpushdate=2022-11-29T14:36:41+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class="tool-buttons not-print"><div id=goto-gtranslate class=button title=å¤šè¯­è¨€ç¿»è¯‘><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=æ·±æµ…æ¨¡å¼åˆ‡æ¢><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=è¿”å›é¡¶éƒ¨><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a>
<a href=https://github.com/peace0phmind rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/images/avatar.png"><meta itemprop=name content="peace0phmind"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="peace0phmind"><meta itemprop=description content></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="Many Factors Affecting Optimization"><meta itemprop=description content="05-many-factors-affecting-optimization"></span><header class="post-header not-print"><h1 class=post-title itemprop="name headline">Many Factors Affecting Optimization
<a href=https://github.com/peace0phmind/peace0phmind.github.io/tree/master/content/posts/202211/05-many-factors-affecting-optimization.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=ç¼–è¾‘><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>å‘è¡¨äºï¼š</span>
<time title="å‘è¡¨äºï¼š2022-11-20 11:22:00 +0800 +0800" itemprop="dateCreated datePublished" datetime="2022-11-20 11:22:00 +0800 +0800">2022-11-20</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>åˆ†ç±»äºï¼š</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/ itemprop=url rel=index><span itemprop=name></span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=å­—æ•°><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>å­—æ•°ï¼š</span><span>6216</span></span>
<span class=post-meta-item title=é˜…è¯»><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>é˜…è¯»ï¼š&ap;</span>
<span>13åˆ†é’Ÿ</span></span>
<span class=post-meta-item title=æµè§ˆ><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>æµè§ˆï¼š</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/posts/202211/05-many-factors-affecting-optimization/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body print autonumber" itemprop=articleBody><h2 id=æœºå™¨å­¦ä¹ çš„ä¸€èˆ¬æ­¥éª¤>æœºå™¨å­¦ä¹ çš„ä¸€èˆ¬æ­¥éª¤</h2><div class=markmap style=height:200px><script type=text/template>---
markmap:
  maxWidth: 300
  colorFreezeLevel: 6
  initialExpandLevel: 10
---

# loss on training data

## large
- [model bias](#model-bias)
  - make your model complex
- [optimization](#optimization-issue)

## small
- loss on testing data
  - large
    - [overfitting](#overfitting)
      - more training data
      - data augmentation
      - make your model simpler
    - [mismatch](#mismatch)
  - small ğŸ˜Š
  
## [model complex trade-off(æƒè¡¡)](#bias-complexity-trade-off)
- Split your training data into training set and validation set for model selection</script></div><h3 id=model-bias>Model Bias</h3><ul><li>The model is too simple.<ul><li>find a needle in a haystack (å¤§æµ·æé’ˆ)</li><li>but there is no needle</li></ul></li><li>Solution: redesign your model to make it more flexible<ul><li>more features</li><li>more neurons, layers</li></ul></li></ul><h3 id=optimization-issue>Optimization Issue</h3><ul><li>Large loss not always imply model bias. There is another possibility &mldr;<ul><li>A needle is in a haystack&mldr;, Just cannot find it.</li></ul></li></ul><h3 id=model-bias-vs-optimization-issue>Model Bias v.s. Optimization Issue</h3><ul><li>Gaining the insights from comparison</li><li>å½“åœ¨æµ‹è¯•æ•°æ®ä¸Šå’Œè®­ç»ƒæ•°æ®ä¸Šæœ‰ç€ç±»ä¼¼çš„lossæ›²çº¿æ—¶ï¼Œè¿™è¯´æ˜æ˜¯<code>Optimization Issue</code>çš„é—®é¢˜</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.009.jpg alt></figure></p><h4 id=optimization-issue-1>Optimization Issue</h4><ul><li>Start from shallower networks(or other models), which are easier to optimize.</li><li>ä»æ›´å®¹æ˜“ä¼˜åŒ–çš„è¾ƒæµ…çš„ç½‘ç»œï¼ˆæˆ–å…¶ä»–æ¨¡å‹ï¼‰å¼€å§‹ã€‚</li><li>If deeper networks do not obtain smaller loss on <code>training data</code>, then there is optimization issue.</li><li>å¦‚æœæ›´æ·±çš„ç½‘ç»œåœ¨â€œè®­ç»ƒæ•°æ®â€ä¸Šæ²¡æœ‰è·å¾—æ›´å°çš„æŸå¤±ï¼Œé‚£ä¹ˆå°±å­˜åœ¨ä¼˜åŒ–é—®é¢˜ã€‚</li></ul><h3 id=overfitting>Overfitting</h3><ul><li>Small loss on training data, large loss on testing data. Why?</li><li>æ•°æ®åˆ†å¸ƒçš„è¿™æ¡è™šçº¿é€šå¸¸æ˜¯æ— æ³•æ˜ç¡®çš„è·çŸ¥çš„ï¼Œæˆ‘ä»¬é€šå¸¸åªèƒ½æ‹¿åˆ°åœ¨è¿™æ¡æ›²çº¿ä¸Šçš„å¤šä¸ª<code>Training Data</code></li><li>ç”±äºmodelçš„Flexible, è®­ç»ƒå‡ºæ¥çš„è¿™ä¸ªæ¨¡å‹ï¼Œåœ¨æ²¡æœ‰è®­ç»ƒæ•°æ®çš„åœ°æ–¹ä¼šæœ‰â€œfreestyleâ€, ä»è€Œå¯¼è‡´æµ‹è¯•æ•°æ®çš„overfitting<ul><li>å¢åŠ è®­ç»ƒæ•°æ®</li><li>Data augmentation(ç”¨ä¸€äº›å¯¹è¿™ä¸ªé—®é¢˜çš„ç†è§£ï¼Œè‡ªå·±åˆ›é€ å‡ºæ–°çš„è®­ç»ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼šå¯¹å›¾ç‰‡å·¦å³åè½¬ï¼Œæˆ–è€…æ˜¯æˆªå–å…¶ä¸­ä¸€å—ç­‰)</li><li>é€šè¿‡é™åˆ¶modelæ¥è§£å†³overfittingï¼Œç»™modelåˆ¶é€ é™åˆ¶çš„æ–¹æ³•ï¼š<ul><li>make your model simpler</li><li>Less parameters, sharing parameters<ul><li>Fully-connectedçš„æ¶æ„æ˜¯ä¸€ä¸ªæ¯”è¾ƒæœ‰å¼¹æ€§çš„æ¶æ„ï¼›è€ŒCNNæ˜¯ä¸€ä¸ªæ¯”è¾ƒæœ‰é™åˆ¶çš„æ¶æ„ï¼ˆæ ¹æ®å½±åƒçš„ç‰¹æ€§æ¥é™åˆ¶æ¨¡å‹çš„å¼¹æ€§ï¼‰</li></ul></li><li>Less features</li><li>Early stopping</li><li>Regularization</li><li>Dropout</li></ul></li><li>è¿™é‡Œéœ€è¦æ³¨æ„ï¼Œå¤ªå¤šçš„é™åˆ¶å’Œå¤ªç®€å•çš„æ¨¡å‹ä¼šå¯¼è‡´<code>model bias</code></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.014.jpg alt></figure></p><h3 id=bias-complexity-trade-off>Bias-Complexity Trade-off</h3><ul><li>é€šè¿‡è§‚å¯Ÿ<code>Training loss</code>å’Œ<code>Testing loss</code>çš„lossæ›²çº¿æ¥é€‰æ‹©modelå’Œå¯¹åº”çš„æ¨¡å‹é™åˆ¶</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.020.jpg alt></figure></p><h3 id=n-fold-cross-validation>N-fold Cross Validation</h3><ul><li>Cross Validationå°±æ˜¯N-flod Cross Validationçš„ä¸€ä¸ªç‰¹ä¾‹</li><li>å¦‚æœä½¿ç”¨<code>Cross Validation</code>, åˆ™ä½¿ç”¨<code>Validation Set</code>çš„lossæœ€å°è¿›è¡Œæ¨¡å‹çš„é€‰æ‹©</li><li>å½“ä½¿ç”¨<code>N-fold Cross Validation</code>æ—¶ï¼Œåˆ™ä½¿ç”¨mseçš„avgæœ€å°æ¥æŒ‘é€‰æ¨¡å‹</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02.024.jpg alt></figure></p><h3 id=mismatch>Mismatch</h3><ul><li>Your training and testing data have different distributions.</li><li>éœ€è¦å¯¹è®­ç»ƒèµ„æ–™å’Œæµ‹è¯•èµ„æ–™æœ‰ä¸€å®šçš„äº†è§£æ‰èƒ½åˆ†æ¸…åˆ°åº•æ˜¯ä¸æ˜¯mismatch</li><li>mismatchå’Œoverfittingä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œoverfittingå¯ä»¥é€šè¿‡å¢åŠ è®­ç»ƒèµ„æ–™æ¥è§£å†³ï¼Œè€Œmismatchæ— æ³•é€šè¿‡å¢åŠ è®­ç»ƒèµ„æ–™æ¥è§£å†³</li></ul><h2 id=optimization-fails-because>Optimization Fails because</h2><ul><li>loss is <code>Not small enough</code>, because the gradient is close to zero.</li><li>Gradientä¸ºé›¶çš„æƒ…å†µæœ‰ï¼š<code>local minima</code>, <code>local maxima</code>, <code>saddle point</code>ç­‰</li><li><code>saddle point</code>: Gradientä¸ºé›¶, åŒæ—¶æ—¢ä¸æ˜¯<code>local minima</code>ä¹Ÿä¸æ˜¯<code>local maxima</code>çš„åœ°æ–¹</li><li>Gradientä¸ºé›¶çš„ç‚¹ç»Ÿç§°ä¸º<code>critical point</code></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.002.jpg alt></figure></p><h3 id=tayler-series-approximationæ³°å‹’çº§æ•°é€¼è¿‘>Tayler Series Approximation(æ³°å‹’çº§æ•°é€¼è¿‘)</h3><ul><li>å¦‚ä½•çŸ¥é“ä¸€ä¸ª<code>critical point</code>æ˜¯<code>local minima</code>è¿˜æ˜¯<code>saddle point</code></li><li>å…¶ä¸­åŒ…æ‹¬ Gradient $\color{green}g$ is a <u>vector</u>, Hessian $\color{red}H$ is a <u>matrix</u>.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.004.jpg alt></figure></p><h3 id=hessian>Hessian</h3><ul><li>Gradient $\color{green}g$ ä¸º0æ—¶ï¼Œåˆ™å¯çŸ¥ç›®å‰æ‰€åœ¨ä½ç½®ä¸ºä¸´ç•Œç‚¹<code>Critical Point</code></li><li>Hessian $\color{red}H$ can telling the properties of critical points.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.005.jpg alt></figure></p><ul><li>å½“$\color{red}H$è¿™ä¸ªçŸ©é˜µä¸­çš„å€¼å…¨éƒ¨ä¸ºæ­£å€¼ï¼Œåˆ™å½“å‰æ‰€åœ¨ä¸º<code>Local Minima</code></li><li>å½“$\color{red}H$è¿™ä¸ªçŸ©é˜µä¸­çš„å€¼å…¨éƒ¨ä¸ºè´Ÿå€¼ï¼Œåˆ™å½“å‰æ‰€åœ¨ä¸º<code>Local Maxima</code></li><li>å½“$\color{red}H$è¿™ä¸ªçŸ©é˜µä¸­çš„å€¼æœ‰æ­£æœ‰è´Ÿï¼Œåˆ™å½“å‰æ‰€åœ¨ä¸º<code>Saddle Point</code></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.006.jpg alt></figure></p><h3 id=saddle-point-vs-local-minima>Saddle Point v.s. Local Minima</h3><ul><li>åœ¨ä¸€ç»´çš„ç©ºé—´ä¸­çœ‹åˆ°çš„local minimaï¼Œåœ¨äºŒç»´çš„ç©ºé—´ä¸­çœ‹åˆ°çš„å¯èƒ½å°±åªæ˜¯saddle point.</li><li>å½“æˆ‘ä»¬æœ‰æ›´å¤šçš„å‚æ•°ï¼Œä¹Ÿè®¸local minimaæ˜¯å¾ˆå°‘è§çš„</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.014.jpg alt></figure></p><h4 id=minimum-ratio>Minimum Ratio</h4><ul><li>æ˜¯æ‰€æœ‰<code>Local Minima</code>çš„æ•°é‡ä¸æ‰€æœ‰<code>Critical Point</code>çš„æ¯”å€¼</li><li>ä»å›¾ä¸Šå¯çŸ¥ï¼Œæœ€å¤§çš„ratioä¹Ÿåªæ˜¯0.6</li><li>å›¾ä¸ŠEigen Valueså°±æ˜¯å‰æ–‡æ‰€è¯´çš„Hessian Matrix.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-1.015.jpg alt></figure></p><h2 id=batch>Batch</h2><ul><li>ä¸ä¼šæ‹¿æ‰€æœ‰çš„èµ„æ–™å»ç®—å¾®åˆ†ï¼Œä¼šæŠŠæ‰€æœ‰çš„èµ„æ–™åˆ†æˆå¾ˆå¤šä¸ªbatchï¼Œ</li><li>æ¯ä¸ªbatchçš„èµ„æ–™ç®—ä¸€ä¸ªlossï¼Œç®—ä¸€ä¸ªGradientå†updateå‚æ•°</li><li>æ‰€æœ‰çš„èµ„æ–™ç®—è¿‡ä¸€éå«åšä¸€ä¸ªepoch</li><li><code>shaffle</code> after each epoch, <code>shaffle</code>æœ‰å¾ˆå¤šä¸åŒçš„åšæ³•ï¼š<ul><li>ä¸€ä¸ªå¸¸è§çš„åšæ³•æ˜¯åœ¨æ¯ä¸ªepochå¼€å§‹ä¹‹å‰ï¼Œä¼šåˆ†ä¸€æ¬¡batchï¼Œæ¯ä¸€ä¸ªepochçš„batchéƒ½ä¸ä¸€æ ·</li></ul></li></ul><h3 id=small-batch-vs-large-batch>Small Batch v.s. Large Batch</h3><ul><li>Consider we have 20 examples(N=20)</li><li>Batch size = N (Full batch)<ul><li>Update after seeing all the 20 examples</li><li>Long time for cooldown but powerful</li></ul></li><li>Batch size = 1<ul><li>Update for each example, Update 20 times in an epoch</li><li>Short time for cooldown but noisy</li></ul></li></ul><h4 id=ç”±äºæœ‰gpuçš„å¹³è¡Œè¿ç®—çš„èƒ½åŠ›ä»æ€§èƒ½çš„è§’åº¦å‡ºå‘å¾—åˆ°å¦‚ä¸‹ç»“è®º>ç”±äºæœ‰GPUçš„å¹³è¡Œè¿ç®—çš„èƒ½åŠ›ï¼Œä»æ€§èƒ½çš„è§’åº¦å‡ºå‘å¾—åˆ°å¦‚ä¸‹ç»“è®ºï¼š</h4><ul><li>Larger batch size does not require longer time to compute gradient(unless batch size is too large)</li><li>Smaller batch requires longer time for one epoch (longer time for seeing all data once)</li></ul><h4 id=ä»å‡†ç¡®ç‡æ¥çœ‹>ä»å‡†ç¡®ç‡æ¥çœ‹</h4><ul><li>åè€Œæ˜¯æœ‰noisyçš„batchå¯ä»¥å¾—åˆ°å¥½çš„ç»“æœã€‚Smaller batch size has better performance.</li><li>å¦‚ä¸‹å›¾ï¼Œæ¨ªè½´æ˜¯batch sizeï¼Œçºµè½´æ˜¯æ­£ç¡®ç‡ã€‚å¦‚å›¾å¯çŸ¥batch sizeè¶Šå¤§ï¼Œvalidation setä¸Šçš„ç»“æœè¶Šå·®ã€‚</li><li>è¿™ä¸ªæ˜¯overfittingä¹ˆï¼Ÿè¿™ä¸ªä¸æ˜¯overfitting, å› ä¸ºæˆ‘ä»¬ç”¨çš„æ•°æ®å’Œæ¨¡å‹éƒ½æ˜¯ä¸€è‡´çš„ã€‚æ‰€ä»¥è¿™é‡Œå‘ç”Ÿåœ¨larger batch sizeä¸Šçš„æƒ…å†µæ˜¯Optimization Fails.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.008.jpg alt></figure></p><p>ä¸ºä»€ä¹ˆåœ¨Noisyçš„batch sizeä¸Šupdateæ›´å¥½å‘¢ï¼Ÿä¸€ç§å¯èƒ½çš„è§£é‡Šæ˜¯ï¼š</p><ul><li>Full Batchæ¯”è¾ƒå®¹æ˜“stuckï¼Œè€ŒSmall Batchç”±äºä¸åŒbatchçš„æ•°æ®æœ‰æ‰€ä¸åŒï¼Œæ‰€ä»¥ç›¸å¯¹æ¥è¯´ä¸å¤ªå®¹æ˜“stuckï¼Œæ›´å®¹æ˜“trainåˆ°æ¯”è¾ƒå°çš„loss</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.009.jpg alt></figure></p><p>æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå°çš„batch sizeä¸ä»…é’ˆå¯¹trainingæœ‰æ•ˆï¼Œåœ¨testingçš„æ—¶å€™ä¹Ÿæ¯”å¤§çš„batch sizeè¦å¥½ã€‚å¦‚ä¸‹å›¾ï¼š</p><ul><li>æ•°æ®ç›¸åŒï¼Œæ¨¡å‹ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå°†å¤§çš„batch sizeåœ¨training setä¸Šçš„accuracyè°ƒæ•´çš„å’Œå°çš„batch sizeä¸€æ ·</li><li>è€Œä»å›¾ä¸Šå³ä¾§è¡¨æ ¼è§‚å¯Ÿï¼ŒLBçš„accuracyæ¯”SBçš„accuracyè¦å·®ï¼Œè¿™æ˜¯overfitting</li><li>è¯¦è§èµ„æ–™ï¼š
<a href=https://arxiv.org/abs/1609.04836 title="On Large-Batch Training for deep Learning: Generalization Gap and Sharp Minima" rel="noopener external nofollow noreferrer" target=_blank class=exturl>On Large-Batch Training for deep Learning: Generalization Gap and Sharp Minima
<i class="fa fa-external-link-alt"></i></a></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.010.jpg alt></figure></p><p>ä¸ºä»€ä¹ˆä¼šæœ‰è¿™ç§ç°è±¡å‘¢ï¼Ÿ</p><ul><li>å‡è®¾å¦‚ä¸‹å›¾çš„training lossä¸Šæœ‰å¾ˆå¤šä¸ªLocal Minimaï¼Œè¿™äº›Local Minimaçš„Losséƒ½è¶³å¤Ÿå°</li><li>ä½†æ˜¯Local Minimaè¿˜æ˜¯æœ‰å¥½åä¹‹åˆ†çš„ã€‚å¦‚å›¾ä¸­ï¼ŒFlat minimaï¼ˆç›†åœ°ï¼‰çš„å®¹é”™æ€§è¦ä¼˜äºsharp minimaï¼ˆå³¡è°·ï¼‰ã€‚</li><li>å¤§çš„batch sizeå€¾å‘äºèµ°åˆ°å³¡è°·é‡Œé¢ï¼Œè€Œå°çš„batch sizeå€¾å‘äºèµ°åˆ°ç›†åœ°é‡Œé¢ã€‚</li><li>å°çš„batch sizeæœ‰å¾ˆå¤šçš„noisyï¼Œå®ƒæ¯æ¬¡èµ°çš„æ–¹å‘éƒ½ä¸å¤ªä¸€æ ·ï¼Œå¦‚æœè¿™ä¸ªå³¡è°·æ¯”è¾ƒçš„çª„ï¼Œé‚£ä¹ˆnoisyçš„batch sizeå¾ˆå®¹æ˜“è·³å‡ºå³¡è°·ã€‚</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.011.jpg alt></figure></p><table><thead><tr><th>â€‚</th><th>Small</th><th>Large</th></tr></thead><tbody><tr><td>Speed for one update (no parallel)</td><td>Faster</td><td>Slower</td></tr><tr><td>speed for one update (with parallel)</td><td>Same</td><td>Same(not too large)</td></tr><tr><td>Time for one epoch</td><td>Slower</td><td>Faster</td></tr><tr><td>Gradient</td><td>Noisy</td><td>Stable</td></tr><tr><td>Optimization</td><td>Better</td><td>Worse</td></tr><tr><td>Generalization</td><td>Better</td><td>Worse</td></tr></tbody></table><span style=color:red>Batch size is a hyperparameter you have to decide.</span><h4 id=å…¼é¡¾é€Ÿåº¦ä¸generalizationçš„ç ”ç©¶æ–‡ç« >å…¼é¡¾é€Ÿåº¦ä¸Generalizationçš„ç ”ç©¶æ–‡ç« </h4><ul><li><a href=https://arxiv.org/abs/1904.00962 title="Large Batch Optimization for Deep Learning: Training BERT in 76 minutes" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1711.04325 title="Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/2001.02312 title="Stochastic Weight Averaging in Parallel: Large-Batch Training that Generalizes Well" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Stochastic Weight Averaging in Parallel: Large-Batch Training that Generalizes Well
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1708.03888 title="Large Batch Training of Convolutional Networks" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Large Batch Training of Convolutional Networks
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1706.02677 title="Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour
<i class="fa fa-external-link-alt"></i></a></li></ul><h2 id=momentum>Momentum</h2><h3 id=vanilla-gradient-descent>(Vanilla) Gradient Descent</h3><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.016.jpg alt></figure></p><h3 id=gradient-descent--momentum>Gradient Descent + Momentum</h3><ul><li>Movement: movement of last step minus gradient at present</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-2.017.jpg alt></figure></p><h2 id=adaptive-learning-rate>Adaptive Learning Rate</h2><ul><li>$ \text{Training stuck} \ne \text{Small Gradient} $</li><li>å½“lossä¸å†ä¸‹é™çš„æ—¶å€™ï¼Œéœ€è¦ç¡®è®¤ä¸€ä¸‹Gradientæ˜¯å¦ä¸º0ï¼›å³lossä¸å†ä¸‹é™éœ€è¦åˆ†æstuckçš„åŸå› </li><li>å¦‚å›¾ï¼Œå½“lossä¸å†ä¸‹é™æ—¶ï¼Œnorm of gradient å¹¶æ²¡æœ‰ä¸º0</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.002.jpg alt></figure></p><p>Training can be difficult even without critical points.</p><ul><li>Learning rate cannot be one-size-fits-all(ä¸€åˆ€åˆ‡).</li><li>Different parameters needs different learning rate.</li><li>ç›¸å¯¹å¹³å¦çš„Gradient Descentéœ€è¦è¾ƒå¤§çš„Learning Rate</li><li>ç›¸å¯¹å°–é”çš„Gradient Descentéœ€è¦è¾ƒå°çš„Learning Rate</li></ul><p>Formulation for <code>one</code> parameter:</p><p>\begin{align}
\theta_i^{t+1} & \leftarrow \theta_i^t - {\color{red}\eta}g_i^t \cr
g_i^t & = \frac{\partial L}{\partial \theta_i} |_{\theta=\theta^t} \cr
& \Downarrow \cr
\theta_i^{t+1} & \leftarrow \theta_i^t - {\color{red}\frac{\eta}{\sigma_i^t}}g_i^t
\end{align}</p><p>${\color{red}\frac{\eta}{\sigma_i^t}}$å°±æ˜¯<code>Parameter dependent</code>çš„Learning Rate,ä¸‹é¢ä»‹ç»å‡ ç§å¸¸è§çš„è®¡ç®—æ–¹æ³•ï¼š</p><h3 id=root-mean-square-used-in-adagrad>Root Mean Square (Used in Adagrad)</h3><p>\begin{align}
\theta_i^1 & \leftarrow \theta_i^0 - \frac{\eta}{\sigma_i^0}g_i^0 & \sigma_i^0 &= \sqrt{(g_i^0)^2} = |g_i^0| \cr
\theta_i^2 & \leftarrow \theta_i^1 - \frac{\eta}{\sigma_i^1}g_i^1 & \sigma_i^1 &= \sqrt{\frac{1}{2}[(g_i^0)^2+(g_i^1)^2]} \cr
\theta_i^3 & \leftarrow \theta_i^2 - \frac{\eta}{\sigma_i^2}g_i^2 & \sigma_i^2 &= \sqrt{\frac{1}{3}[(g_i^0)^2+(g_i^1)^2+(g_i^2)^2]} \cr
& \vdots \cr
\theta_i^{t+1} & \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t & \sigma_i^t &= \sqrt{\frac{1}{t+1}\sum_{i=0}^t(g_i^t)^2}
\end{align}</p><ul><li>å°çš„$\sigma_i^t$ä¼šæœ‰å¤§çš„step</li><li>å¤§çš„$\sigma_i^t$ä¼šæœ‰å°çš„step</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.007.jpg alt></figure></p><h3 id=learning-rate-adapts-dynamically>Learning rate adapts dynamically</h3><ul><li>å³ä½¿é’ˆå¯¹åŒä¸€ä¸ªå‚æ•°ï¼Œåœ¨ä¸åŒçš„æ—¶å€™ï¼Œå¯èƒ½ä¹Ÿéœ€è¦æœ‰ä¸åŒçš„Learning Rate</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.008.jpg alt></figure></p><h4 id=rmsprop>RMSProp</h4><p>\begin{align}
\theta_i^1 & \leftarrow \theta_i^0 - \frac{\eta}{\sigma_i^0}g_i^0 & \sigma_i^0 &= \sqrt{(g_i^0)^2} = |g_i^0| \cr
& & & \text{è®¾ } 0 &lt; \alpha &lt; 1 \cr
\theta_i^2 & \leftarrow \theta_i^1 - \frac{\eta}{\sigma_i^1}g_i^1 & \sigma_i^1 &= \sqrt{\alpha(\sigma_i^0)^2 + (1-\alpha)(g_i^1)^2} \cr
\theta_i^3 & \leftarrow \theta_i^2 - \frac{\eta}{\sigma_i^2}g_i^2 & \sigma_i^2 &= \sqrt{\alpha(\sigma_i^1)^2 + (1-\alpha)(g_i^2)^2]} \cr
& \vdots \cr
\theta_i^{t+1} & \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t & \sigma_i^t &= \sqrt{\alpha(\sigma_i^{t-1})^2 + (1-\alpha)(g_i^t)^2}
\end{align}</p><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.010.jpg alt></figure></p><h4 id=adam-rmsprop--momentum>Adam: RMSProp + Momentum</h4><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.011.jpg alt></figure></p><h3 id=learning-rate-scheduling>Learning Rate Scheduling</h3><ul><li>Learning Rate Decay<ul><li>After the training goes, we are closer to the destination, so we reduce the learning rate.</li></ul></li><li>Warm Up<ul><li>Increase and then decrease?</li><li><a href=https://arxiv.org/abs/1512.03385 title="Deep Residual Learning for Image Recognition" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Deep Residual Learning for Image Recognition
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Attention Is All You Need
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1908.03265 title="On the Variance of the Adaptive Learning Rate and Beyond" rel="noopener external nofollow noreferrer" target=_blank class=exturl>On the Variance of the Adaptive Learning Rate and Beyond
<i class="fa fa-external-link-alt"></i></a></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.016.jpg alt></figure></p><h2 id=summary-of-optimization>Summary Of Optimization</h2><ul><li>Momentum: weighted sum of the previous gradients (è€ƒè™‘æ–¹å‘)</li><li>$\sigma_i^t$: åªè€ƒè™‘å¤§å°ä¸è€ƒè™‘æ–¹å‘</li><li>$\eta^t$: Learning rate scheduling</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-3.017.jpg alt></figure></p><h2 id=loss-for-classification>Loss for Classification</h2><h3 id=class-as-one-hot-vector>Class as one-hot vector</h3><ul><li>one-hot vector for multi-output</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.005.jpg alt></figure></p><h3 id=soft-max>Soft-max</h3><ul><li>soft-maxå¯¹ä¸Šå±‚å¤šè¾“å‡ºç»“æœåšä¸€ä¸ªnormalize</li><li>å¹¶ä¸”è®©å¤§çš„å€¼å’Œå°çš„å€¼ä¹‹é—´çš„å·®è·æ›´å¤§</li><li>soft-maxæœ‰æ—¶åˆå«åšlogit</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.007.jpg alt></figure></p><h3 id=loss-function>Loss function</h3><ul><li>Mean Square Error (MSE): $ e = \sum\limits_i(\hat y_i - y_i^\prime)^2 $</li><li>Cross-entropy: $ e = -\sum\limits_i \hat y_i ln y_i^\prime $</li><li>Minimizing cross-entropy is equivalent to maximizing likelihood.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.008.jpg alt></figure></p><h3 id=loss-function-affect-optimization>Loss function affect Optimization</h3><ul><li>è®¾ç›®å‰åšä¸€ä¸ªä¸‰åˆ†ç±»çš„æ¨¡å‹ï¼Œå½“å‰è¿™ä¸ªåˆ†ç±»çš„ç»“æœæ˜¯$ \hat y = \begin{bmatrix}1 \cr 0 \cr 0 \end{bmatrix} $</li><li>eè¡¨ç¤º$yåˆ°\hat y$ä¹‹é—´çš„è·ç¦»ï¼Œå¯ä»¥æ˜¯MSEï¼Œä¹Ÿå¯ä»¥æ˜¯Cross-entropy</li><li>$y_1$çš„å–å€¼èŒƒå›´ä¸º[-10, 10], $y_2$çš„å–å€¼èŒƒå›´ä¸º[-10, 10], $y_3$çš„å–å€¼ä¸ºå›ºå®šå€¼-1000</li><li>ä¸‹å›¾å·¦å³åˆ†åˆ«ä¸ºMSEå’ŒCross-entropyæƒ³å¯¹äºyçš„å–å€¼çš„Error Surface,</li><li>è¿™ä¸¤å¼ å›¾ä¸­Error Surfaceçš„ç‰¹ç‚¹éƒ½æ˜¯å³ä¸‹è§’losså°ï¼Œå·¦ä¸Šè§’losså¤§</li><li>å‡è®¾æˆ‘ä»¬å¼€å§‹çš„åœ°æ–¹éƒ½æ˜¯å·¦ä¸Šè§’ï¼šå¦‚æœæˆ‘ä»¬é€‰æ‹©Cross-entropyï¼Œå·¦ä¸Šè§’çš„åœ°æ–¹æ˜¯æœ‰æ–œç‡çš„,æ‰€ä»¥å¯ä»¥é€šè¿‡gradientçš„æ–¹æ³•ä¸€è·¯å‘å³ä¸‹è§’èµ°è¾¾åˆ°small loss;
å¦‚æœæˆ‘ä»¬é€‰æ‹©MSEï¼Œæˆ‘ä»¬å°±å¡ä½äº†ï¼Œåœ¨MSEå·¦ä¸Šè§’è¿™ä¸ªlosså¾ˆå¤§çš„åœ°æ–¹ï¼Œå®ƒçš„gradientéå¸¸å°ï¼Œè¶‹è¿›äºé›¶ï¼Œè€Œè·ç¦»ç›®æ ‡åˆå¾ˆè¿œï¼Œæ²¡æœ‰å¾ˆå¥½çš„åŠæ³•é€šè¿‡gradientçš„æ–¹æ³•èµ°åˆ°å³ä¸‹è§’ã€‚</li><li>æ‰€ä»¥å¦‚æœåšclassificationæ—¶ï¼Œé€‰æ‹©ä½¿ç”¨MSEåšlossæ—¶ï¼Œæœ‰å¾ˆå¤§å¯èƒ½æ€§trainä¸èµ·æ¥; å½“ç„¶å¦‚æœä½¿ç”¨ç±»ä¼¼Adamè¿™äº›å¥½çš„Optimizeræ—¶ï¼Œä¹Ÿè®¸æœ‰æœºä¼šèµ°åˆ°å³ä¸‹è§’ã€‚</li><li><span style=color:red>Changing the loss function can change the difficulty of optimization.</span></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-4.009.jpg alt></figure></p><h2 id=normalization>Normalization</h2><h3 id=changing-landscape>Changing Landscape</h3><ul><li>$w_1, w_2$ä¸ä¸åŒçš„featureç›¸å…³ï¼Œç”±äºä¸åŒçš„featureèŒƒå›´ä¸åŒï¼Œå¯¼è‡´äº†$w_1, w_2$çš„å˜åŠ¨å¯¹æœ€ç»ˆçš„lossäº§ç”Ÿä¸åŒçš„å½±å“</li><li>æ˜¯å¦å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªæ–¹æ³•ï¼Œè®©ä¸åŒçš„featureæœ‰ç€ç›¸ä¼¼çš„range</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.003.jpg alt></figure></p><h3 id=feature-normalization>Feature Normalization</h3><ul><li>ä¸‹å›¾åªæ˜¯Feature Normalizationçš„ä¸€ç§å¯èƒ½</li><li>$ x^1, x^2, \cdots, x^R $ ä¸ºæ•°æ®çš„Rä¸ªfeatures</li><li>For each dimension i:<ul><li>$m_i$ : mean</li><li>$\sigma_i$ : standard deviation</li></ul></li><li>Normalization: $\tilde{x}_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i} $<ul><li>The means of all dims are 0, and the variances are all 1</li></ul></li><li>In general, feature normalization makes gradient descent converge faster.</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.004.jpg alt></figure></p><h4 id=thetaçš„normalization>$\theta$çš„Normalization</h4><ul><li>$\tilde{x}^1$åœ¨ç»è¿‡$W^1$åå¾—åˆ°çš„$z^1$ä¹Ÿæ˜¯å…·æœ‰ä¸åŒçš„rangeçš„ï¼Œè¿™å°†å¯¼è‡´é’ˆå¯¹$W^2$çš„optimizeä¼šæ¯”è¾ƒçš„å›°éš¾</li><li>å¯¹äº$W^2$æ¥è¯´ï¼Œè¿™é‡Œçš„zæˆ–è€…aä¹Ÿæ˜¯featureï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦è€ƒè™‘å¯¹zæˆ–è€…aåšNormalization</li><li>é‚£ä¹ˆåˆ°åº•åœ¨æ¿€æ´»å‡½æ•°çš„å‰é¢è¿˜æ˜¯åé¢åšnormalizationå‘¢ï¼Ÿå®ä½œä¸­éƒ½å¯ä»¥ï¼Œä½†å½“activation functionä¸ºSigmoidæ—¶ï¼Œå»ºè®®åœ¨Sigmoidçš„å‰é¢ï¼Œä¹Ÿå°±æ˜¯è¿™é‡Œçš„zåšnormalizationã€‚</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.005.jpg alt></figure></p><h4 id=feature-normalizationçš„è®¡ç®—>Feature Normalizationçš„è®¡ç®—</h4><ul><li>$\mu = \frac{1}{n}\sum\limits_{i=1}^nz^i $</li><li>$ \sigma = \sqrt{\frac{1}{n}\sum\limits_{i=1}^n(z^i-\mu)^2} $</li><li>$ \tilde{z}^i = \frac{z^i-\mu}{\sigma} $</li></ul><h4 id=batch-normalization>Batch normalization</h4><ul><li>ç”±äºéœ€è¦å¯¹$x, z^1, z^2, \cdots, z^n$éƒ½åšnormalizationï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„networkï¼Œå¦‚æœä½¿ç”¨è¿™ä¸ªå·¨å¤§çš„networké’ˆå¯¹æ‰€æœ‰çš„æ•°æ®å»æ±‚$\muå’Œ\sigma$æ˜¯ä¸å¤ªç°å®çš„ï¼Œ
æ‰€ä»¥åªèƒ½å°†èŒƒå›´ç¼©å°åˆ°ä¸€ä¸ªbatchï¼Œä»è€Œè¯ç”Ÿäº†batch normalizationã€‚</li><li>è€Œåœ¨åšBatch normalizationçš„æ—¶å€™ï¼Œå¾€å¾€è¿˜ä¼šåŠ ä¸Šä¸€ä¸ª$\betaå’Œ\gamma$ï¼Œ å¾—åˆ°æ–¹ç¨‹$ \hat{z}^i = \gamma\odot\tilde{z}^i+\beta $ å…¶ä¸­ $ \tilde{z}^i = \frac{z^i-\mu}{\sigma} $</li><li>å®é™…åœ¨åšè®­ç»ƒæ—¶ï¼Œè€ƒè™‘åˆ°normalizationçš„æƒ…å†µï¼Œåœ¨åˆå§‹æƒ…å†µä¸‹$\gamma$ä¼šè¢«åˆå§‹åŒ–ä¸ºone vector(å…¨1çš„å‘é‡), è€Œ$\beta$ä¼šè¢«åˆå§‹åŒ–ä¸ºzero vector(å…¨0çš„å‘é‡)ã€‚</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/02-5.008.jpg alt></figure></p><h4 id=batch-normalization---testing>Batch normalization - Testing</h4><ul><li>We do not always have batch at testing stage.</li><li>Computing the moving average of $\mu$ and $\sigma$ of the batches during training.</li><li>å¦‚ä¸‹è®¡ç®—å‡º$\bar{\mu}, \bar{\sigma}$ï¼Œæ›¿æ¢trainingä¸­çš„è¡¨è¾¾å¼$ \tilde{z} = \frac{z-\mu}{\sigma} $å¾—åˆ°$ \tilde{z} = \frac{z-\bar{\mu}}{\bar{\sigma}} $</li><li>How to compute moving average?</li></ul><p>\begin{align}
\mu^1, \mu^2, \mu^3, \cdots, \mu^t \cr
\bar{\mu} \leftarrow p\bar{\mu} + (1-p)\mu^t
\end{align}</p><h4 id=batch-normalization-refs>Batch Normalization Refs</h4><ul><li><a href=https://arxiv.org/abs/1502.03167 title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1805.11604 title="How Does Batch Normalization Help Optimization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>How Does Batch Normalization Help Optimization
<i class="fa fa-external-link-alt"></i></a><ul><li>Experimental results (and theoretically analysis) support batch normalization change the landscape of error surface.</li><li>This suggests that the positive impact of BatchNorm on training might be somewhat serendipitous(å¶ç„¶çš„ï¼Œå‘ç°äº†ä¸€ä¸ªæ„æ–™ä¹‹å¤–çš„ä¸œè¥¿).</li></ul></li></ul><h3 id=normalization-refs>Normalization Refs</h3><ul><li><a href=https://arxiv.org/abs/1702.03275 title="Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1607.06450 title="Layer Normalization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Layer Normalization
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1607.08022 title="Instance Normalization: The Missing Ingredient for Fast Stylization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Instance Normalization: The Missing Ingredient for Fast Stylization
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1803.08494 title="Group Normalization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Group Normalization
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1602.07868 title="Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks
<i class="fa fa-external-link-alt"></i></a></li><li><a href=https://arxiv.org/abs/1705.10941 title="Spectral Norm Regularization for Improving the Generalizability of Deep Learning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Spectral Norm Regularization for Improving the Generalizability of Deep Learning
<i class="fa fa-external-link-alt"></i></a></li></ul><h2 id=new-optimization>New Optimization</h2><h3 id=what-you-have-known-before>What you have known before?</h3><ul><li>SGD - stochastic gradient descent: éšæœºæ¢¯åº¦ä¸‹é™</li><li>SGDM - stochastic gradient descent with momentum</li><li>Adagrad</li><li>RMSProp</li><li>Adam</li></ul><h4 id=some-notations>Some Notations</h4><ul><li>$\theta_t$: model parameters at time step t</li><li>$\nabla L(\theta_t) \text{or } g_t$: gradient at $\theta_t$, used to compute $\theta_{t+1}$</li><li>$m_{t+1}$: momentum accumulated from time step 0 to time step t, which is used to compute $\theta_{t+1}$</li></ul><p>\begin{align}
g_t \cr
\overleftarrow{x_t \rightarrow \theta_t \rightarrow y_t ^\underleftrightarrow{L(\theta_t;x_t)} \hat{y}_t}
\end{align}</p><h4 id=what-is-optimization-about->What is Optimization about ?</h4><ul><li>Find a $\theta$ to get the lowest $\sum_x L(\theta; x)$ !!</li><li>Or, Find a $\theta$ to get the lowest $L(\theta)$ !!</li></ul><h4 id=on-line-vs-off-line-learning>On-line vs Off-line learning</h4><ul><li>On-line: one pair of $(x_t, \hat{y}_t)$ at a time step</li><li>Off-line: pour all $(x_t, \hat{y}_t)$ into the model at every time step</li></ul><h4 id=sgd-stochastic-gradient-descent>SGD (stochastic gradient descent)</h4><ul><li>Start at position $\theta^0$</li><li>Compute gradient at $\theta^0$</li><li>Move to $\theta^1 = \theta^0 - \eta \nabla L(\theta^0)$</li><li>Compute gradient at $\theta^1$</li><li>Move to $\theta^2 = \theta^1 - \eta \nabla L(\theta^1)$</li><li>Stop until $\nabla L(\theta^t) \approx 0$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.010.jpg alt></figure></p><h4 id=sgdm-stochastic-gradient-descent-with-momentum>SGDM (stochastic gradient descent with momentum)</h4><ul><li>Start at point $\theta^0$</li><li>Movement $V^0=0$</li><li>Compute gradient at $\theta^0$</li><li>Movement $V^1=\lambda V^0 - \eta \nabla L(\theta^0)$</li><li>Move to $\theta^1 = \theta^0 + V^1$</li><li>Compute gradient at $\theta^1$</li><li>Movement $V^2 = \lambda V^1 - \eta \nabla L(\theta^1)$</li><li>Move to $\theta^2 = \theta^1 + V^2$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.011.jpg alt></figure></p><h4 id=adagrad>Adagrad</h4><ul><li>$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\sum_{i=0}^{t-1}(g_i)^2}}g_{t-1}$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.014.jpg alt></figure></p><h4 id=rmsprop-1>RMSProp</h4><ul><li>$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t}}g_{t-1}$</li><li>$v_1 = g_0^2$</li><li>$v_t = \alpha v_{t-1} + (1-\alpha)(g_{t-1})^2$</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.015.jpg alt></figure></p><h4 id=adam>Adam</h4><ul><li>$ \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \varepsilon}\hat{m}_t $</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.016.jpg alt></figure></p><h3 id=optimizers-real-application>Optimizers: Real Application</h3><ul><li>BERT: ä½œQ/Aï¼Œ æ–‡ç« ç”Ÿæˆï¼Œ 2018å¹´æå‡ºï¼Œä½¿ç”¨ ADAM</li><li>Transformer: ç”¨äºç¿»è¯‘ï¼Œ ä½¿ç”¨ ADAM</li><li>Tacotron: æœ€æ—©ä½¿ç”¨ç¥ç»ç½‘ç»œä½œé€¼çœŸçš„è¯­éŸ³ç”Ÿæˆçš„æ¨¡å‹, 2017å¹´æå‡º, ä½¿ç”¨ ADAM</li><li>Yolo: ä½¿ç”¨ SGDM</li><li>Mask R-CNN: ä½¿ç”¨ SGDM</li><li>ResNet: ä½¿ç”¨ SGDM</li><li>Gig-GAN: ç”Ÿæˆå½±åƒï¼Œ ä½¿ç”¨ ADAM</li><li>MEMO: åœ¨ä¸åŒçš„åˆ†ç±»ä»»åŠ¡ä¸­å­¦åˆ°å…±åŒçš„èµ„è®¯ï¼Œ ä½¿ç”¨ ADAM</li></ul><h3 id=adam-vs-sgdm>Adam vs SGDM</h3><ul><li>Adam: fast training, large generalization gap, unstable</li><li>SGDM: stable, little generalization gap, better convergence(?)</li></ul><h4 id=simply-combine-adam-with-sgdm>Simply combine Adam with SGDM?</h4><ul><li>SWATS: Begin with Adam(fast), end with SGDM</li></ul><h4 id=towards-improving-adam>Towards improving Adam</h4><h5 id=troubleshooting>Troubleshooting</h5><ul><li>æ ¹æ®å‰é¢çš„è®¡ç®—å…¬å¼ï¼Œä»100000åˆ°100998çš„æ¢¯åº¦éƒ½æ˜¯1,æ‰€ä»¥movementéƒ½æ˜¯$\eta$</li><li>ç¬¬100999æ­¥çš„æ¢¯åº¦å¾ˆå¤§ï¼Œ100000,å¯¹åº”çš„movementä¸º$10\sqrt{10}\eta$</li><li>è¿™å¯¼è‡´äº†å‰é¢å¾ˆå¤šæ— ç”¨çš„æ‰¹æ¬¡çš„æ¢¯åº¦äº§ç”Ÿäº†çº¦1000$\eta$çš„æ— ç”¨å˜åŒ–ï¼ˆä¹±èµ°ï¼‰ï¼Œè€Œå…¶ä¸­ä¸€ä¸ªæœ‰ç”¨çš„å¾ˆå°çš„æ‰¹æ¬¡å¸¦æ¥çš„å˜åŒ–åªæœ‰33ä¸ª$\eta$</li><li>å½“æ¢¯åº¦å¤§éƒ¨åˆ†éƒ½å¾ˆå°æ—¶ï¼Œå°±ä¼šäº§ç”Ÿè¿™æ ·çš„é—®é¢˜ï¼›æœ‰ç”¨çš„å¤§çš„æ¢¯åº¦å¯¹åº”çš„æ‰¹æ¬¡ä¼šè¢«å¤§é‡çš„å°çš„æ— ç”¨çš„æ¢¯åº¦ç‰µç€é¼»å­èµ°</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.029.jpg alt></figure></p><h5 id=amsgrad>AMSGrad</h5><ul><li>Reduce the influence of non-informative gradients</li><li>Remove de-biasing due to the max operation</li><li>è¿™ä¸ªç®—æ³•çš„æ”¹è¿›å¯ä»¥ç±»æ¯”ä¸ºAdagradå’ŒRMSProp, æ‰€ä»¥æ„Ÿè§‰å¹¶æ²¡æœ‰èµ·åˆ°å¾ˆå¥½çš„æ•ˆæœ</li></ul><h5 id=troubleshooting-1>Troubleshooting</h5><ul><li>In the final stage of training, most gradients are small and non-informative, while some mini-batches provide large informative gradient rarely</li><li>Learning rates are either extremely large(for small gradients) or extremely small(for large gradients)</li></ul><h5 id=adabound>AdaBound</h5><ul><li>AMSGrad only handles large learning rates</li><li>AdaBoundçš„å…¬å¼ä¸­ï¼Œæœ‰å‚æ•°å¹¶éadaptiveçš„ï¼Œè€Œæ˜¯æœ‰ç‚¹å·¥ç¨‹æ–¹æ³•</li></ul><h4 id=towards-improving-sgdm>Towards Improving SGDM</h4><ul><li>Adaptive learning rate algorithms: dynamically adjust learning rate over time</li><li>SGD-type algorithms: fix learning rate for all updates&mldr; too slow for small learning rates and bad result for large learning rates</li><li>There might be a &ldquo;best&rdquo; learning rate?</li></ul><h4 id=learning-rate-range-test>Learning Rate range test</h4><ul><li>Learning Rate åœ¨å¾ˆå¤§æˆ–å¾ˆå°çš„æ—¶å€™ï¼Œæ€§èƒ½éƒ½ä¸ä¼šå¾ˆå¥½</li><li>Learning Rate é€‚ä¸­çš„æ—¶å€™ï¼Œæ€§èƒ½æ‰æ¯”è¾ƒå¥½</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.035.jpg alt></figure></p><h5 id=cyclical-lr>Cyclical LR</h5><ul><li>learning rate: decide by LR range test</li><li>step size: several epochs</li><li>avoid local minimum by varying learning rate</li><li>learning rateåœ¨å¤§å°ï¼Œå¤§å°çš„å¾ªç¯è¿›è¡Œå˜åŒ–</li><li>å˜å¤§çš„æ—¶å€™æ—¶åœ¨ä½œexploration(æ¢ç´¢)ï¼Œå˜å°çš„æ—¶å€™æ˜¯åœ¨ä½œæ”¶æ•›</li><li>The more exploration the better!</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.036.jpg alt></figure></p><h5 id=sgdr>SGDR</h5><ul><li>ä¸ç”¨è±¡Cyclical LRä¸€æ ·ä¸æ–­çš„å˜å¤§å†å˜å°ï¼Œè€Œæ˜¯åœ¨å˜å°åï¼Œé‡æ–°å˜å›åˆå§‹å€¼é‡æ–°å¼€å§‹</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.037.jpg alt></figure></p><h5 id=one-cycle-lr>One-Cycle LR</h5><ul><li>warm-up + annealing + fine-tuning</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.038.jpg alt></figure></p><h3 id=does-adam-need-warm-up>Does Adam need warm-up?</h3><ul><li>distorted(æ‰­æ›²çš„) gradient -> distorted EMA squared gradients -> Bad learning rate</li><li>keep your step size small at the beginning of training helps to reduce the variance of the gradients</li><li>æ–°çš„warm-upçš„æ–¹æ³•æ˜¯ï¼Œå…ˆå˜å°ï¼Œå†å˜å¤§</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.040.jpg alt></figure></p><h4 id=radam>RAdam</h4><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.041.jpg alt></figure></p><h3 id=radam-vs-swats>RAdam vs SWATS</h3><table><thead><tr><th>Â </th><th>RAdam</th><th>SWATS</th></tr></thead><tbody><tr><td>Inspiration<br>çµæ„Ÿ</td><td>Distortion of gradient at the beginning of training results in inaccurate adaptive learning rate<br>è®­ç»ƒå¼€å§‹æ—¶æ¢¯åº¦å¤±çœŸå¯¼è‡´è‡ªé€‚åº”å­¦ä¹ ç‡ä¸å‡†ç¡®</td><td>non-convergence and generalization gap of Adam, slow training of SGDM<br>Adam çš„ä¸æ”¶æ•›å’Œæ³›åŒ–å·®è·ï¼ŒSGDM è®­ç»ƒç¼“æ…¢</td></tr><tr><td>How</td><td>Apply warm-up learning rate to reduce the influence of inaccurate adaptive learning rate<br>åº”ç”¨é¢„çƒ­å­¦ä¹ ç‡å‡å°‘è‡ªé€‚åº”å­¦ä¹ ç‡ä¸å‡†ç¡®çš„å½±å“</td><td>Combine their advantages by applying Adam first, then SGDM<br>é€šè¿‡å…ˆåº”ç”¨ Adamï¼Œç„¶ååº”ç”¨ SGDM æ¥ç»“åˆå®ƒä»¬çš„ä¼˜åŠ¿</td></tr><tr><td>Switch</td><td>SGDM to RAdam</td><td>Adam to SGDM</td></tr><tr><td>Why switch</td><td>The approximation of the variance of $\hat{v}_t$ is invalid at the beginning of training<br>æ–¹å·®çš„è¿‘ä¼¼å€¼$\hat{v}_t$åœ¨è®­ç»ƒå¼€å§‹æ—¶æ— æ•ˆ</td><td>To purse better convergence<br>è¿½æ±‚æ›´å¥½çš„æ”¶æ•›</td></tr><tr><td>Switch point</td><td>When the approximation becomes valid<br>å½“è¿‘ä¼¼å€¼æˆç«‹æ—¶</td><td>Some human-defined criteria<br>ä¸€äº›äººä¸ºå®šä¹‰çš„æ ‡å‡†</td></tr></tbody></table><h3 id=k-step-forward-1-step-back>K step forward, 1 step back</h3><ul><li>Lookahead: universal wrapper for all optimizers</li><li>è¿™ä¸ªç®—æ³•æœ‰ä¸¤ç»„weightï¼š<ul><li>è¿™é‡Œ$\theta$æ˜¯ç”¨äºexplore(æ¢ç´¢)çš„ï¼Œå«åšFast weights</li><li>$\phi$æ˜¯çœŸæ­£éœ€è¦çš„weightï¼Œå«åšSlow weights</li></ul></li><li>å¾ªç¯åˆ†å¤–å¾ªç¯å’Œå†…å¾ªç¯ã€‚<ul><li>å†…å¾ªç¯ä¼šèµ°kæ­¥ï¼Œå†…å¾ªç¯çš„Optimå¯ä»¥ä½¿ç”¨ä»»æ„çš„optimizer</li><li>æ¯èµ°å®Œä¸€éå†…å¾ªç¯ï¼Œæ ¹æ®å½“å‰ä½ç½®ï¼Œåˆ°å†…å¾ªç¯å¼€å§‹å‰çš„ä½ç½®ï¼Œæ ¹æ®$\alpha$è®¡ç®—å‡ºä¸‹æ¬¡å¾ªç¯å¼€å§‹çš„ä½ç½®</li><li>æ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ–°è®¡ç®—å‡ºæ¥çš„$\phi$è¿›è¡Œæ–°ä¸€è½®çš„å†…å¾ªç¯</li></ul></li><li>è¿™ä¸ªæ–¹æ³•å’ŒMemoé‡Œé¢çš„æ¼”ç®—æ³•Reptileå¾ˆåƒ</li></ul><p>\begin{align}
& \text{For }t = 1, 2, \dots \text{(outer loop)} \cr
& â€ƒ\theta_{t,0} = \phi_{t-1} \cr
& â€ƒ\text{For } i = 1, 2, \dots, k \text{(inner loop)} \cr
& â€ƒ â€ƒ\theta_{t,i} = \theta_{t, i-1} + \text{Optim(Loss, data, }\theta_{t, i-1}\text{)} \cr
& â€ƒ\phi_t = \phi_{t-1} + \alpha(\theta_{t,k} - \phi_{t-1})
\end{align}</p><ul><li>1 step back: avoid too dangerous exploration</li><li>Look for a more flatten minimum</li><li>More stable</li><li>Better generalization</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/05-many-factors-affecting-optimization/03.044.jpg alt></figure></p><h2 id=reference-video>Reference Video</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/WeHM2xpYQpw style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/QW6uINn7uGk style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/zzbr1h9sF54 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/HYUXEeh3kwY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/O2VkP8dJ5FE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/BABPWOkSbLE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/1_HBTJyWgNA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/4pUmZ8hXlHM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/e03YKGHXnL8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></div><footer class="post-footer not-print"><div class=post-tags><a href=/tags/optimization>optimization</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/202211/docker-swarm/ rel=next title="Docker Swarm"><i class="fa fa-chevron-left"></i> Docker Swarm</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/202211/04-logistic-regression/ rel=prev title="Logistic Regression">Logistic Regression
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class="footer not-print"><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>peace0phmind</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"https://peace0phmind.github.io","i18n":{"ds_day":" å¤©å‰","ds_days":" å¤© ","ds_hour":" å°æ—¶å‰","ds_hours":" å°æ—¶ ","ds_just":"åˆšåˆš","ds_min":" åˆ†é’Ÿå‰","ds_mins":" åˆ†é’Ÿ","ds_month":" ä¸ªæœˆå‰","ds_years":" å¹´ ","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits":"","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","placeholder":"æœç´¢..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.0","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"è¯·æ–‡æ˜å‘è¨€å“Ÿ ãƒ¾(â‰§â–½â‰¦*)o","reaction":true,"reactiontext":["ç‚¹èµ","è¸©ä¸€ä¸‹","å¾—æ„","ä¸å±‘","å°´å°¬","ç¡è§‰"],"reactiontitle":"ä½ è®¤ä¸ºè¿™ç¯‡æ–‡ç« æ€ä¹ˆæ ·ï¼Ÿ","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"å¿«æ¥å‘è¡¨ä½ çš„æ„è§å§ (â‰§âˆ€â‰¦)ã‚","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.c000895f35c4f549795bbb87a678888fa0246f9513d456814f0988441a9a4b7c.js defer></script>
<script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin=anonymous></script>
<script type=text/javascript>mediumZoom("[data-zoomable]",{background:null})</script><script type=text/javascript>window.MathJax={options:{ignoreHtmlClass:"markmap"},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!1,packages:{"[+]":["base","extpfeil","ams","amscd","newcommand","knowl","sfrac"]}},chtml:{},loader:{load:["input/asciimath","[tex]/extpfeil","[tex]/amscd","[tex]/newcommand","[pretext]/mathjaxknowl3.js"],paths:{pretext:"https://pretextbook.org/js/lib"}},startup:{ready(){const e=MathJax._.input.tex.Configuration.Configuration,t=MathJax._.input.tex.SymbolMap.CommandMap;new t("sfrac",{sfrac:"SFrac"},{SFrac(e,t){const n=e.ParseArg(t),s=e.ParseArg(t),o=e.create("node","mfrac",[n,s],{bevelled:!0});e.Push(o)}}),e.create("sfrac",{handler:{macro:["sfrac"]}}),MathJax.startup.defaultReady()}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js integrity crossorigin=anonymous></script><style>.markmap>svg{width:100%;height:100%}</style><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader@0.14.4-alpha.1 integrity crossorigin=anonymous d></script></body></html>