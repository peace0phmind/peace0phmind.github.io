<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.111.3"><link rel="shortcut icon" type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/x-icon href=/images/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/images/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/images/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/images/icons/apple_touch_icon_next.png><meta itemprop=name content="Fat vs Deep Network"><meta itemprop=description content="06-fat-vs-deep-network"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://peace0phmind.github.io/images/avatar.png"><meta itemprop=keywords content="fat,deep,network"><meta property="og:type" content="article"><meta property="og:title" content="Fat vs Deep Network"><meta property="og:description" content="06-fat-vs-deep-network"><meta property="og:image" content="/images/avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="https://peace0phmind.github.io/posts/202211/06-fat-vs-deep-network/"><meta property="og:site_name" content="Mind's Home"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="peace0phmind"><meta property="article:published_time" content="2022-11-24 10:01:58 +0800 +0800"><meta property="article:modified_time" content="2022-11-24 10:01:58 +0800 +0800"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.dabf51c13eb53dd19c627065c0fe53422155aaa07efc790ec29ea1d5b51707b8.css><style type=text/css>.post-footer{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){console.log("aaaa hello world!");const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"06-fat-vs-deep-network","permalink":"https://peace0phmind.github.io/posts/202211/06-fat-vs-deep-network/","title":"Fat vs Deep Network","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>Fat vs Deep Network - Mind's Home</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class="header not-print" itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Mind's Home</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Notebook</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#全量空间与样本空间>全量空间与样本空间</a><ul><li><a href=#how-to-make--pd_train-text-is-bad-smaller->How to make $ P(D_{train} \text{ is bad}) smaller $</a></li><li><a href=#if-we-want--pd_train-text-is-bad--leq-delta->If we want $ P(D_{train} \text{ is bad}) \leq \delta $</a></li><li><a href=#tradeoff权衡-of-model-complexity>Tradeoff(权衡) of Model Complexity</a></li></ul></li><li><a href=#why-hidden-layer>Why Hidden Layer?</a><ul><li><a href=#deeper-is-better>Deeper is Better?</a></li><li><a href=#why-we-need-deep->Why we need deep ?</a></li><li><a href=#analogy---logic-circuits>Analogy - Logic Circuits</a></li><li><a href=#use-neuron-network>Use Neuron Network</a><ul><li><a href=#22-pieces>$2^2$ pieces</a></li><li><a href=#23-pieces>$2^3$ pieces</a></li><li><a href=#2k-pieces>$2^k$ pieces</a></li></ul></li><li><a href=#think-more>Think more</a></li></ul></li><li><a href=#i-used-a-validation-set-but-my-model-still-overfitted>I used a validation set, but my model still overfitted</a></li><li><a href=#can-shallow-network-fit-any-function>Can shallow network fit any function</a><ul><li><a href=#deep-vs-shallow>deep vs shallow</a></li><li><a href=#universality>Universality</a></li></ul></li><li><a href=#potential-of-deep>Potential of Deep</a><ul><li><a href=#why-we-need-deep>Why we need deep?</a></li><li><a href=#upper-bound-of-linear-pieces>Upper Bound of Linear Pieces</a></li><li><a href=#lower-bound-of-linear-pieces>Lower Bound of Linear Pieces</a></li><li><a href=#使用mnist进行实验得到如下结论>使用MNIST进行实验，得到如下结论</a></li></ul></li><li><a href=#using-deep-structure-to-fit-functions>Using deep structure to fit functions</a><ul><li><a href=#假设需要fit一个简单的function--fx--x--2>假设需要fit一个简单的function $ f(x) = x ^ 2$</a></li><li><a href=#why-care-about--yx2->Why care about $ y=x^2 $</a></li><li><a href=#deep-better-than-shallow>Deep better than Shallow</a></li></ul></li><li><a href=#reference-video>Reference video</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=peace0phmind src=/images/img-lazy-loading.gif data-src=/images/avatar.png><p class=site-author-name itemprop=name>peace0phmind</p><div class=site-description itemprop=description></div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/posts/><span class=site-state-item-count>39</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>2</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>25</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/peace0phmind title="Github → https://github.com/peace0phmind" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2020-10-20T19:37:14+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=35176></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=93></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-03-17T16:09:51+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class="tool-buttons not-print"><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a>
<a href=https://github.com/peace0phmind rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://peace0phmind.github.io/posts/202211/06-fat-vs-deep-network/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/images/avatar.png"><meta itemprop=name content="peace0phmind"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="peace0phmind"><meta itemprop=description content></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="Fat vs Deep Network"><meta itemprop=description content="06-fat-vs-deep-network"></span><header class="post-header not-print"><h1 class=post-title itemprop="name headline">Fat vs Deep Network
<a href=https://github.com/peace0phmind/peace0phmind.github.io/tree/master/content/posts/202211/06-fat-vs-deep-network.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-11-24 10:01:58 +0800 +0800" itemprop="dateCreated datePublished" datetime="2022-11-24 10:01:58 +0800 +0800">2022-11-24</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/ml itemprop=url rel=index><span itemprop=name>ml</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>3365</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>7分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/posts/202211/06-fat-vs-deep-network/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body print autonumber" itemprop=articleBody><h2 id=全量空间与样本空间>全量空间与样本空间</h2><ul><li>全量空间(理想状态)： If we can collect all datasets in the universe $ D_{all} $, we can find the best threshold $h^{all}$<ul><li>$ h^{all} = \text{arg}\min\limits_h L (h, D_{all}) $</li></ul></li><li>样本空间(现实发生)： We only collect some examples $D_{train}$ from $D_{all}$<ul><li>$ D_{train} = {(x^1, \hat{y}^1), (x^2, \hat{y}^2), \dots, (x^N, \hat{y}^N)} $</li><li>$ (x^n, \hat{y}^n) \sim D_{all} $, 采样满足independently and identically distributed (i.i.d.)特性</li><li>通过$ D_{train} $找到最小Loss的那个h，叫$ h^{train} $： $ h^{train} = \text{arg}\min\limits_h L(h, D_{train}) $</li></ul></li><li>In most applications, you cannot obtain $ D_{all}$. (Testing data $D_{test}$ as the proxy of $D_{all}$)</li></ul><p>We hope $ L({\color{blue}h^{train}}, {\color{red}D_{all}}) $ and $ L({\color{red}h^{all}}, {\color{red}D_{all}}) $ are close.</p><p>We want $ L({\color{blue}h^{train}}, {\color{red}D_{all}}) - L({\color{red}h^{all}}, {\color{red}D_{all}}) \leq \delta $</p><h3 id=how-to-make--pd_train-text-is-bad-smaller->How to make $ P(D_{train} \text{ is bad}) smaller $</h3><ul><li>$ P(D_{train} \text{ is bad}) = |H| \cdot 2exp(-2N\varepsilon^2) $</li><li>Larger N and smaller $ |H| $, 样本空间数量尽可能大，全量空间数量尽可能小</li></ul><h3 id=if-we-want--pd_train-text-is-bad--leq-delta->If we want $ P(D_{train} \text{ is bad}) \leq \delta $</h3><ul><li>How many training examples do we need?</li><li>$ |H|\cdot 2exp(-2N\varepsilon^2) \leq \delta \Rightarrow N \geq \frac{log(2|H|/\delta)}{2\varepsilon^2} $</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 假设 H = 10000, delta = 0.1, epsilon = 0.1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span><span style=color:#ae81ff>10000</span><span style=color:#f92672>/</span><span style=color:#ae81ff>0.1</span>, math<span style=color:#f92672>.</span>e)<span style=color:#f92672>/</span>(<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span><span style=color:#ae81ff>0.1</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># output is: 610.3036322765086</span>
</span></span></code></pre></div><h3 id=tradeoff权衡-of-model-complexity>Tradeoff(权衡) of Model Complexity</h3><ul><li>Larger N and smaller $|H| \Rightarrow L(h^{train}, D_{all}) - L(h^{all}, D_{all}) \leq \delta $</li><li>Smaller $ |H| \Rightarrow \text{Larger }L(h^{all}, D_{all})$</li></ul><h2 id=why-hidden-layer>Why Hidden Layer?</h2><ul><li>Piecewise Linear<ul><li>We can have good approximation with sufficient pieces.</li><li>piecewise linear = constant + sum of a set of <code>Hard Sigmoid</code></li><li>or use two <code>Rectified Linear Unit (ReLU)</code> instead of one <code>Hard Sigmoid</code></li></ul></li><li>一层的Piecewise Linear就可以模拟出任何的函数，那么为何需要多层呢？</li><li>Why we want &ldquo;Deep&rdquo; network, not &ldquo;Fat&rdquo; network?</li></ul><h3 id=deeper-is-better>Deeper is Better?</h3><ul><li>下面列出了层数与正确率的列表，左边Thin + Tall;右边Fat + Short</li><li>表格中5X2k的参数量与3772相当，所以放在一起做个比较</li><li>从表格中可以的出结论，在参数量相当的情况下，瘦高型要好于矮胖型</li></ul><table><thead><tr><th>Layer X Size</th><th>Word Error Rate(%)</th><th>Layer X Size</th><th>Word Error Rate(%)</th></tr></thead><tbody><tr><td>1 X 2k</td><td>24.2</td><td></td><td></td></tr><tr><td>2 X 2k</td><td>20.4</td><td></td><td></td></tr><tr><td>3 X 2k</td><td>18.4</td><td></td><td></td></tr><tr><td>4 X 2k</td><td>17.8</td><td></td><td></td></tr><tr><td>5 X 2k</td><td>17.2</td><td>1 X 3772</td><td>22.5</td></tr><tr><td>7 X 2k</td><td>17.1</td><td>1 X 4634</td><td>22.6</td></tr><tr><td></td><td></td><td>1 X 16K</td><td>22.1</td></tr></tbody></table><h3 id=why-we-need-deep->Why we need deep ?</h3><ul><li>yes, one hidden layer can represent any function.</li><li>However, using deep structure is more effective.</li><li>产生相同的function，Shallow的参数数量要多于Deep的。</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/03.014.jpg alt></figure></p><h3 id=analogy---logic-circuits>Analogy - Logic Circuits</h3><ul><li>parity check (奇偶校验)<ul><li>For input sequence with <code>d</code> bits, 假设此处d=4。</li><li>Two-layer circuit need O($2^d$) gates: O = 16</li><li>或者，3个XNOR的gates也可以达到相同的效果</li><li>With multiple layers, we need only O(d) gates：O = 4</li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/03.015.jpg alt></figure></p><h3 id=use-neuron-network>Use Neuron Network</h3><h4 id=22-pieces>$2^2$ pieces</h4><ul><li>如图，假设图中所用activation function为ReLU</li><li>图中所画图形需要旋转90度，将x作为横轴来看</li><li>x与$a_1$的关系是，当x从0-1时，$a_1$先下降(从1-0)后上升(从0-1)</li><li>$a_1$与$a_2$的关系是，当$a_1$从0-1时，$a_2$先下降(从1-0)后上升(从0-1)</li><li>x与$a_2$的关系, 会得到4个线段</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/03.020.jpg alt></figure></p><h4 id=23-pieces>$2^3$ pieces</h4><ul><li>接上步， x与$a_2$的关系, 会得到4个线段</li><li>$a_2$与$a_3$的关系是，当$a_2$从0-1时，$a_3$先下降(从1-0)后上升(从0-1)</li><li>x与$a_3$的关系, 会得到8个线段</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/03.021.jpg alt></figure></p><h4 id=2k-pieces>$2^k$ pieces</h4><ul><li>假设在x从0-1的变化过程中，需要让一个nn的output y有$2^k$的线段<ul><li>使用deep的方式，那么只需要k层，每层2个neurons，总共2K个neurons就可以满足需要</li><li>使用shallow的方式，那么需要$2^k$个neurons才能满足需要</li></ul></li><li>所以要产生同样的function<ul><li>Deep: 参数量比较小，smaller $|H|$; 模型比较简单</li><li>Shallow: 参数量比较大，larger $|H|$；模型比较复杂</li><li>在样本数相同的情况下，复杂的模型比较容易overfitting</li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/03.022.jpg alt></figure></p><h3 id=think-more>Think more</h3><ul><li>Deep networks outperforms shallow ones when the required functions are <code>complex and regular</code>.<ul><li>Image, speech, etc. have this characteristics.</li></ul></li><li>当所需功能“复杂且有规律”时，深度网络优于浅层网络。</li><li>Deep is exponentially better than shallow even when $ y = x^2 $</li></ul><h2 id=i-used-a-validation-set-but-my-model-still-overfitted>I used a validation set, but my model still overfitted</h2><ul><li>什么时候理想和现实会有差距<ul><li>抽到一个不好的training（validation）data的时候，会有较大的差距（overfitting）</li><li>模型比较复杂</li><li>待选择的模型太多了，也可能overfitting</li></ul></li></ul><h2 id=can-shallow-network-fit-any-function>Can shallow network fit any function</h2><ul><li>network structure: 网络架构，决定了网络怎么连接</li><li>同样的structure，填入不同的参数（填入不同的weight和bias），得到不同的function，即一个function space(/set)</li></ul><h3 id=deep-vs-shallow>deep vs shallow</h3><ul><li>假设一个function: $ f(x) = 2(2cos^2(x)-1)^2-1 $, 如下图</li><li>图中不同颜色的线段代表不同的hidden层数</li><li>横坐标表示units(/parameters)的数量，纵坐标表示对应的loss</li><li>从图中可以看出在相同参数量的情况下，deep越深，loss越小（fit越好）</li><li>相应的当观察相同loss的情况下，deep越深参数量越小</li><li>注：unit的数目就是neuron的数目，用neuron的数目表示一个network的架构，neuron的数目和parameter的数目是正相关的</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.003.jpg alt></figure></p><ul><li>假设一个目标function： $ y = x^2 $</li><li>假设一个small的shallow的function，可能并不能很好的fit到这个function，只有当这个shallow的参数足够large的时候，才能找到有效的function</li><li>而deep去fit同样的function，需要的参数是少的</li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.005.jpg alt></figure></p><h3 id=universality>Universality</h3><ul><li>Given a shallow network structure with one hidden layer with ReLU activation and linear output</li><li>Given a L-Lipschitz function $f^*$<ul><li>How many neurons are needed to approximate $f^*$?<ul><li>L-Lipschitz Function (smooth)</li><li>$ || f(x_1) - f(x_2) || \le L || x_1 - x_2 || $</li><li>左边是Output change, 右边是Input change</li><li>L = 1 for &ldquo;1-Lipschitz&rdquo; function; 当L取1时，式子表示为:$ || f(x_1) - f(x_2) || \le || x_1 - x_2 || $, 即输出的变化不能大于输入的变化</li><li>L = 2 for &ldquo;2 - Lipschitz&rdquo; function</li><li>下图，蓝色变化比较快的就不是1-Lipschitz function,而绿色的是1-Lipschitz function</li></ul></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.008.jpg alt></figure></p><ul><li>Given a L-Lipschitz function $f^*$<ul><li>How many neurons are needed to approximate $f^*$?<ul><li>$ f \in N(K) \Rightarrow {\color{green}\text{The function space defined by the network with K neurons.}} $</li><li>Given a small number $\epsilon > 0$, what is the number of K such that</li><li>Exist $ f \in N(K), \max\limits_{0 \le x \le 1}|f(x) - f^*(x)| \le \epsilon $</li><li>The difference between $f(x)$ and $f^*(x)$ is smaller than $\epsilon$.</li><li>All the functions in N(K) are piecewise linear.</li><li>Approximate $f^*$ by a piecewise linear function f</li></ul></li><li>How to make the $ errors \le \epsilon $<ul><li>如下图：$ l = ||x_1 - x_2 || $, $ error = || f(x_1) - f(x_2) || $</li><li>由L-Lipschitz function得到: $ || f(x_1) - f(x_2) || \le L || x_1 - x_2 || \le \epsilon \Rightarrow error \le l \times L \le \epsilon \Rightarrow error \le \epsilon $</li><li>则： $ l \times L \le \epsilon \Rightarrow l \le \epsilon / L $</li></ul></li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.010.jpg alt></figure></p><ul><li>结合上面推到，下面图可以得到如下结论：<ul><li>当x的取值是[0, 1]： $ segments = L/\epsilon $</li><li>每一段L-Lipschitz function是由2个ReLU neurons组成，即： $ \text{relu neurons} = 2L/\epsilon $</li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.014.jpg alt></figure></p><h2 id=potential-of-deep>Potential of Deep</h2><h3 id=why-we-need-deep>Why we need deep?</h3><ul><li>ReLU networks can represent piecewise linear functions<ul><li>same number of parameters with Shallow & Wide will have Less pieces.</li><li>same number of parameters with Deep & Narrow will have More pieces.</li></ul></li></ul><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.020.jpg alt></figure></p><h3 id=upper-bound-of-linear-pieces>Upper Bound of Linear Pieces</h3><ul><li>Each &ldquo;activation pattern&rdquo; defines a linear function</li><li>ReLU的network，每一个neuron有两个operation的region;一个region的output是0，另一个region的input等于output。</li><li>activation pattern: 某一种neuron的mode的组合</li><li>假设有n个neuron，且每个neuron为并联(shallow & wide)，则最大有n+1个piecewise</li><li>假设有n个neuron，且每个neuron为串联(deep & narrow)，则最大有$2^N$个piecewise</li></ul><p>下面代码说明shallow & wide的网络的piece数(upper_bound_test.py):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>upper_bound_test</span>(x):
</span></span><span style=display:flex><span>    y1 <span style=color:#f92672>=</span> relu(x <span style=color:#f92672>*</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    y2 <span style=color:#f92672>=</span> relu(x <span style=color:#f92672>*</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    y3 <span style=color:#f92672>=</span> relu(x <span style=color:#f92672>*</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (y1 <span style=color:#f92672>+</span> y2 <span style=color:#f92672>+</span> y3) <span style=color:#f92672>*</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(start<span style=color:#f92672>=-</span><span style=color:#ae81ff>2</span>, stop<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num<span style=color:#f92672>=</span><span style=color:#ae81ff>41</span>)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> upper_bound_test(x)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(x, y)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p>下面代码说明deep & narrow的网络的piece数(abs_activation_test.py):</p><ul><li>每个layer有2个ReLU组成一个abs activation function, 每个layer有2个region</li><li>当有3层layer时，有$2^3$的region(piecewise linear)</li><li>deep成立的先决条件是，同样的pattern，反复的出现</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>abs_activation</span>(x):
</span></span><span style=display:flex><span>    y1 <span style=color:#f92672>=</span> relu(x <span style=color:#f92672>*</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>    y2 <span style=color:#f92672>=</span> relu(x <span style=color:#f92672>*</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (y1 <span style=color:#f92672>+</span> y2) <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    layer_number <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(start<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stop<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, num<span style=color:#f92672>=</span><span style=color:#ae81ff>1001</span>)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(layer_number):
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> abs_activation(y)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(x, y)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=lower-bound-of-linear-pieces>Lower Bound of Linear Pieces</h3><ul><li>If K is width, H is depth. We can have at least $K^H$ pieces.</li></ul><h3 id=使用mnist进行实验得到如下结论>使用MNIST进行实验，得到如下结论</h3><ul><li>当宽度固定，不断的增加深度，则piece按照指数递增</li><li>当层数固定，而增加每层的宽度，piece的增加并不明显</li><li>当输入是一个二维的圆圈，在一个100个layer上的network得到的piece，每一层都会是一个对称的图形</li><li>越接近输入端的layer就越重要</li></ul><h2 id=using-deep-structure-to-fit-functions>Using deep structure to fit functions</h2><h3 id=假设需要fit一个简单的function--fx--x--2>假设需要fit一个简单的function $ f(x) = x ^ 2$</h3><ul><li>$f_m(x)=2^m$, a function with $2^m$ pieces</li><li>$ \max\limits_{0 \le x \le 1} | f(x) - f_m(x) | \le \epsilon $</li><li>the minimum m is: $ m \ge -\frac{1}{2}log_2\epsilon -1 $</li></ul><p>$ f(x) = x ^ 2$对应的是下图两个线段相减</p><p><figure><img data-zoomable src=/images/img-lazy-loading.gif data-src=/images/202211/06-fat-vs-deep-network/01.033.jpg alt></figure></p><h3 id=why-care-about--yx2->Why care about $ y=x^2 $</h3><ul><li>$ y=x^2 $ : Square Net</li><li>有了<code>Square Net</code>，就有了<code>Multiply Net</code>。$ y = x_1 x_2 = \frac{1}{2}((x_1+x_2)^2 - x_1^2 - x_2^2) $ 。运用三个<code>Square Net</code>，可以得到一个<code>Multiply Net</code>。</li><li>有了<code>Square Net</code>和<code>Multiply Net</code>就可以做<code>Polynomial(多项式)</code>。</li></ul><h3 id=deep-better-than-shallow>Deep better than Shallow</h3><ul><li>The Power of Depth for Feedforward Neural Networks, COLT, 2016:<ul><li>A function expressible by a 3-layer feedforward network cannot be approximated by 2-layer network.<ul><li>Unless the width of 2-layer network is VERY large</li><li>Applied on activation functions beyond relu<ul><li>The width of 3-layer network is K.</li><li>The width of 2-layer network should be $Ae^{BK^{\sfrac{4}{19}}}$</li></ul></li></ul></li></ul></li><li>Benefits of depth in neural networks, COLT, 2016.<ul><li>A function expressible by a deep feedforward network cannot be approximated by a shallow netowrk.<ul><li>Unless the width of the shallow network is VERY large</li><li>Applied on activation functions beyond relu</li></ul></li></ul></li><li>Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks, ICML, 2017<ul><li>一个球形，3-layer, width 100， is better than 2-layer, width 800</li></ul></li><li>Error bounds for approximations with deep ReLU networks, arXiv, 2016</li><li>Optimal approximation of continuous functions by very deep ReLU nertworks, arXiv 2018</li><li>Why Deep Neural Networks for function Approximation?, ICLR, 2017</li><li>Depth-width Tradeoffs in Approximating Natural Functions with Neural Networks, ICML, 2017</li><li>When and Why Are Deep Networks Better Than Shallow Ones?, AAAI, 2017</li></ul><h2 id=reference-video>Reference video</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/_j9MVVcvyZI style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/yXd2D5J0QDU style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/xQXh3fSvD1A style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/KKT2VkTdFyc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/FN8jclCrqY0 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/qpuLxXrHQB4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></div><footer class="post-footer not-print"><div class=post-tags><a href=/tags/fat>fat</a>
<a href=/tags/deep>deep</a>
<a href=/tags/network>network</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/202211/docker-swarm/ rel=next title="Docker Swarm"><i class="fa fa-chevron-left"></i> Docker Swarm</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/202211/05-many-factors-affecting-optimization/ rel=prev title="Many Factors Affecting Optimization">Many Factors Affecting Optimization
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class="footer not-print"><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>peace0phmind</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"https://peace0phmind.github.io","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.0","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.c000895f35c4f549795bbb87a678888fa0246f9513d456814f0988441a9a4b7c.js defer></script>
<script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin=anonymous></script>
<script type=text/javascript>mediumZoom("[data-zoomable]",{background:null})</script><script type=text/javascript>window.MathJax={options:{ignoreHtmlClass:"markmap"},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!1,packages:{"[+]":["base","extpfeil","ams","amscd","newcommand","knowl","sfrac"]}},chtml:{},loader:{load:["input/asciimath","[tex]/extpfeil","[tex]/amscd","[tex]/newcommand","[pretext]/mathjaxknowl3.js"],paths:{pretext:"https://pretextbook.org/js/lib"}},startup:{ready(){const e=MathJax._.input.tex.Configuration.Configuration,t=MathJax._.input.tex.SymbolMap.CommandMap;new t("sfrac",{sfrac:"SFrac"},{SFrac(e,t){const n=e.ParseArg(t),s=e.ParseArg(t),o=e.create("node","mfrac",[n,s],{bevelled:!0});e.Push(o)}}),e.create("sfrac",{handler:{macro:["sfrac"]}}),MathJax.startup.defaultReady()}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js integrity crossorigin=anonymous></script></body></html>