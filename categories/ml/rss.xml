<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Mind's Home</title><link>https://peace0phmind.github.io/categories/ml/</link><description>Recent content in ml on Mind's Home</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Thu, 24 Nov 2022 10:01:58 +0800</lastBuildDate><atom:link href="https://peace0phmind.github.io/categories/ml/rss.xml" rel="self" type="application/rss+xml"/><item><title>Machine Learning</title><link>https://peace0phmind.github.io/posts/202211/01-machine-learning/</link><pubDate>Fri, 04 Nov 2022 08:50:08 +0800</pubDate><guid>https://peace0phmind.github.io/posts/202211/01-machine-learning/</guid><description>Machine Learning是什么 简单的理解就是在输入和输出中找一个函数 Different types of Functions Deep Learning Introduce history (Ups and downs of Deep Learning) 1958: Perceptron (linear model) 1969: Perceptron has limitation 1980: Multi-layer perceptron Do not have significant difference from DNN today 1986: Backpropagation Usually more than 3</description></item><item><title>Fat vs Deep Network</title><link>https://peace0phmind.github.io/posts/202211/06-fat-vs-deep-network/</link><pubDate>Thu, 24 Nov 2022 10:01:58 +0800</pubDate><guid>https://peace0phmind.github.io/posts/202211/06-fat-vs-deep-network/</guid><description>全量空间与样本空间 全量空间(理想状态)： If we can collect all datasets in the universe $ D_{all} $, we can find the best threshold $h^{all}$ $ h^{all} = \text{arg}\min\limits_h L (h, D_{all}) $ 样本空间(现实发生)： We only collect some examples $D_{train}$ from $D_{all}$ $ D_{train} = {(x^1,</description></item><item><title>Many Factors Affecting Optimization</title><link>https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/</link><pubDate>Sun, 20 Nov 2022 11:22:00 +0800</pubDate><guid>https://peace0phmind.github.io/posts/202211/05-many-factors-affecting-optimization/</guid><description>机器学习的一般步骤 Model Bias The model is too simple. find a needle in a haystack (大海捞针) but there is no needle Solution: redesign your model to make it more flexible more features more neurons, layers Optimization Issue Large loss not always imply model bias. There is another possibility &amp;hellip; A needle is in a haystack&amp;hellip;, Just cannot find</description></item><item><title>Logistic Regression</title><link>https://peace0phmind.github.io/posts/202211/04-logistic-regression/</link><pubDate>Sat, 19 Nov 2022 22:23:55 +0800</pubDate><guid>https://peace0phmind.github.io/posts/202211/04-logistic-regression/</guid><description>Function Set We want to find $P_{w,b}(C_1|x)$ $P_{w, b}(C_1|x) = \sigma(z)$ $ z = w \cdot x + b = \sum\limits_iw_ix_i + b $ function set: $f_{w, b}(x) = P_{w, b}(C_1|x)$ // including all different w and b Goodness of a Function 取minima的 对象写成一个function，这个fun</description></item><item><title>Classification</title><link>https://peace0phmind.github.io/posts/202211/03-classification/</link><pubDate>Tue, 08 Nov 2022 14:59:57 +0800</pubDate><guid>https://peace0phmind.github.io/posts/202211/03-classification/</guid><description>Classification: Given options(classes), the function outputs the correct one. Probabilistic Generative Model features and predict target 一共有7个features，其中 Total = HP + Attack + Deffense + SP Atk + Sp Def + Speed predict target: type of pokemon features and predict target How to do Classification 收集Training</description></item><item><title>Regression</title><link>https://peace0phmind.github.io/posts/202211/02-regression/</link><pubDate>Tue, 08 Nov 2022 14:27:33 +0800</pubDate><guid>https://peace0phmind.github.io/posts/202211/02-regression/</guid><description>Regression: Input a vector, the function outputs a scalar. 使用简单模型 预测问题：根据前面的浏览数据，预测后面的浏览量 Function with Unknown Parameters Model: $y = b + wx_1$ $y$(Label): no. of views on 2/26， $x_1$(feature): no. of views on 2/25 $w$(weight) and $b$(bias) are unknown parameters</description></item></channel></rss>